\documentclass{llncs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{unicode}
\usepackage{amsfonts}% 
\usepackage{amssymb}% 
\usepackage{amsmath}% 
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\def\mat#1{\begin{pmatrix}#1\end{pmatrix}}

\def\abs#1{\left|#1\right|}

\begin{document}
\title{Semi-linear equations appearing in the IP1S problem}

Let~$k$ be a finite field of characteristic two and~$R$ be the local
$k$-algebra $R = k[H]/H^d$.
\section{Solving certain semi-linear equations in local algebras}%<<<
We define two $k$-linear endomorphisms of~$R$
by
\begin{gather*}
θ(∑ x_i H^i) \;=\; ∑_{2i ≥ (d-1)} x_{2i-(d-1)} H^i,\\
ω(∑ x_i H^i) \;=\; ∑_{2i ≤ d)} x_{2i+1} H^{i}.
\end{gather*}
and extend the absolute Frobenius automorphism of~$k$ to~$R$ by
\begin{equation}
φ(∑ x_i H^i) \;=\; ∑ x_{i}^2 H^i.
\end{equation}
We also define the \emph{pseudo-inverse} of a non-zero element~$a = a'
H^r$ of~$R$, where $a'$~is invertible, by
\begin{equation}\label{eq:pseudo-inv}
\widehat{a} \;=\; \frac{1}{a'} H^{d-r}.
\end{equation}
(This is well-defined if~$r ≤ d/2$).
The pseudo-inverse of~$a$ generates the annulator ideal of~$a$ in~$R$.
We note that we have the following relations between~$θ$, $ω$ and~$φ$:
\begin{gather*}\label{eq:theta-omega}
φ(a) θ(x) = θ(a^2 x); \quad
φ(a) ω(x) = ω(a^2 x); \quad
% θ(H^{d-2r} x) = H^{d-r} ω(x);
\\
% a \widehat{a} = 0;
\quad a\,·\, \widehat{\,ab\,} = \widehat{b};\quad
φ(\widehat{a}) = \widehat{φ(a)};\quad
θ(\widehat{a^2} x) = \widehat{φ(a)} ω(x).
\end{gather*}


\begin{proposition}\label{prop:contracting}
For any~$b, c, d ∈ R$, the equation
\begin{equation}
φ(x) = b θ(x) + c θ(Hx) + d
\end{equation}
has:
\begin{itemize}
\item zero or two solutions in~$R$ if $b$~is invertible;
\item one solution in~$R$ if $b$~is not invertible.
\end{itemize}
\end{proposition}


\begin{proof}
The map~$x ↦  φ^{-1} (b θ(x) + c θ(Hx) + d) \pmod{H^{d-1}}$ is
contracting for the $H$-adic valuation on~$R/H^{d-1}$. Therefore, the
equation has a unique solution modulo~$H^{d-1}$. Solving for the
coefficient of~$H^{d-1}$ yields:
\begin{itemize}
\item if $b$~is invertible, an Artin-Schreier equation with either zero or
two solutions ;
\item if $b$~is not invertible, a pseudo-linear equation with exactly one
solution.
\end{itemize}
\end{proof}



\begin{proposition}\label{prop:eq-omega}
For any $b, c ∈ R$, the equation
\begin{equation}\label{eq:eq-omega}
φ(x) ≡ b ω(x) + c ω(Hx) + d \pmod{H^r}
\end{equation}
is liftable: if it has solutions modulo~$H^r$, then any solution
modulo~$H^i$ with~$i ≤ r$ lifts to (at least) one solution
modulo~$H^r$.
\end{proposition}


\begin{proof}
Let~$m = \min (v(b), v(c))$. The first~$2m-1$ equations of the system are
of the form~$x_i^2 = c_i x_0 + b_i x_1 + \dots + c_{m} x_{2(i-m)} +
b_m x_{2(i-m)+1} + d_i$; since~$i > 2(i-m)+1$, this is triangular and has
unique solutions. The equations for~$x_{2m-1}$ and~$x_{2m}$ read
\begin{equation}\label{eq:x2m}\begin{array}{llll}
x_{2m-1}^2 &= d_{2m-1} &+ c_{2m-1} x_0 + b_{2m-1} x_1 + \dots
  &+ c_{m} x_{2m-2} + b_{m} x_{2m-1},\\
x_{2m}^2 &= d_{2m} &+ c_{2m} x_0 + b_{2m} x_1 + \dots
  &+ c_{m} x_{2m} + b_{m} x_{2m+1}.\\
\end{array}\end{equation}
If~$b_m ≠ 0$, then the first equation is an Artin-Schreier equation
on~$x_{2m-1}$; else~$c_m ≠ 0$ and the second equation is an
Artin-Schreier equation on~$x_{2m}$. All the equations for~$x_{2m+i}$
with~$i ≥ 1$ have the free variable~$x_{2m+2i}$ or~$x_{2m+2i+1}$.
\end{proof}


\begin{proposition}\label{prop:eq}
The equation
\begin{equation}\label{eq:theta}
a φ(x) = b θ(x) + c θ(Hx) + ad
\end{equation}
is solvable using a polynomial number of field operations.
\end{proposition}

\begin{proof}
Assume that $x$~is a solution of~\eqref{eq:theta}. Then $y = φ^{-1}(a^2)
x$ satisfies~$φ(y) = a^2 φ(x)$ and~$θ(y) = a θ(x)$, and therefore
\begin{equation}\label{eq:theta-y}
φ(y) ≡ b θ(y) + c θ(Hy) + a^2d \pmod{\widehat{a}}.
\end{equation}
Conversely, let~$y$~be a solution of~\eqref{eq:theta-y}. Since~$v(y) ≥
2 v(a)$, we may write~$y = φ^{-1}(a^2) x'$ with~$x' ∈ R$; this is
then a solution of
\begin{equation}\label{eq:theta-x0}
a φ(x') ≡ b θ(x') + c θ(H x') + d - \widehat{a} d'',\quad
\text{for some~$d'' ∈ R$.}
\end{equation}
Write~$x = x' + \widehat{φ^{-1}(a^2)} x''$, where $x''$~is well-defined
modulo~$a^2$. Equation~\eqref{eq:theta} is then equivalent to
\begin{equation}\label{eq:omega-x1}
φ(x'') = b ω(x'') + c ω(H x'') + d'' \pmod{a}.
\end{equation}
This can be solved via Prop.~\ref{prop:eq-omega}.
\end{proof}
% %>>>
\section{Equations of the IP1S problem}%<<<
The equations for the (cyclic) IP1S problem
are given by
\begin{gather*}\label{eq:matrix1}
u α'_1 = α_1 + α_2 φ(x) + θ(x),\\
\numberthis
u α_2 = α'_2 + α'_1 φ(y) + θ(y),\\
u α'_3 = α_3 + α_4 φ(x) + θ(M\,x),\\
u α_4 = α'_4 + α'_3 φ(y) + θ(M\,y).\\
\end{gather*}
Without changing the values~$α_1, α_2$, we may further assume
the divisibility properties: $α'_1 \mid α_2$, $α'_1 \mid
α'_3 \mid α_4$. Let~$β_i ∈ k[H]$ be such that
\begin{equation}\label{eq:beta}
α_2 = α'_1 β_2, \qquad
α_3 = α'_1 β'_3, \qquad
α_4 = α'_3 β_4 = α'_1 β'_3 β_4.
\end{equation}
When the matrices are in reduced form, the first equation
of~\eqref{eq:matrix1} always determines an invertible~$u ∈ R$.

We choose the line transformation
\begin{equation}\label{eq:line-transform}
P = \mat{ β_2 & 1 & 0 & 0 \\ 0 & β_3 & 0 & 1 \\ β'_3 β_4 & β'_3 & β_2 & 1},
\end{equation}
which gives the equations, where~$X = \mat{x\\y}$:
\begin{equation}\label{eq:matrix2}
α'_1 \mat{β_2^2 & 1 \\ β'_3 (β_2 + β_4) & 0 \\ 0 & 0} φ (X)
+ \mat{β_2 & 1 \\ β'_3 & 0 \\ β'_3 β_4 & β_3} θ (X)
+ \mat{0 & 0 \\ 1 & 0 \\ β_2 & 1} θ (MX) =
\mat{ {} \\ ⋮ \\ {} }.
% \mat{α_1 β_2 + α'_2 \\ α_1 β'_3 + α_3 \\
% α_1 β'_3 β_4 + α'_2 β'_3 + α_3 β_2 + α'_4}.
\end{equation}
Let~$Y = \mat{1 & 0 \\ φ^{-1}(β_2)^2 & 1} X$ and note that the
relation~$θ(a^2 x) = φ(a) θ(x)$ implies
\begin{equation}
φ(Y) = \mat{1 & 0 \\ β_2^2 & 1} φ(X), \quad
θ(Y) = \mat{1 & 0 \\ β_2 & 1} θ(X).
\end{equation}
Factoring on the right the matrices from equation~\eqref{eq:matrix2}, we
obtain the following:
\begin{equation}\label{eq:matrix3}
α'_1 \mat{0 & 1 \\β'_3 (β_2 + β_4) & 0 \\ 0 & 0} φ(Y)
+ \mat{0 & 1 \\ β'_3 & 0 \\ β'_3(β_2 + β_4) & β_3} θ(Y)
+ \mat{0 & 0 \\ 1 & 0 \\ 0 & 1} θ(MX) =
\mat{ {} \\ ⋮ \\ {} }.
\end{equation}
In other words, it is enough to solve the following system, where~$α =
α_1$, $β = β'_3$, and~$γ = β_2 + β_4$:
\begin{equation}\label{eq:system}
\left\{ \begin{array}{c@{\;=\;}l@{\qquad}c}
α φ(x) & θ(x) + C & (1)\\
αβγ φ(z) & (β + M_0) θ(z) + θ(Hz) + C' & (2)\\
0 & (β + M_0) θ(x) + θ(Hx) + βγ θ(y) + C'', & (3)\\
\end{array}\right.
\end{equation}
where the various~$C$ are constants in~$R$. As in the proof of
Prop.~\ref{prop:eq}, we write~$x = x' + φ^{-1}(\widehat{α})^2 x''$
where~$x'$ is a solution
of~$α φ(x') ≡ θ(x_0) + C \pmod{\widehat{α}}$. From the first paragraph,
we know that there are at most two values for~$x' \pmod{\widehat{α^2}}$.
Likewise, we write~$z = z' + φ^{-1}(\widehat{(αβγ)^2}) z''$, where~$φ(z')
≡ (β + M_0) θ(z') + θ(H z') + C' \pmod{\widehat{αβγ}}$. The
system~\eqref{eq:system} then reduces to the equations in~$x'', z''$:
\begin{equation}\label{eq:system1}
\left\{ \begin{array}{c@{\;≡\;}ll@{\qquad}c}
φ(x'') & ω(x'') + F(x') &\pmod{α} & (1)\\
φ(z'') & (β + M_0)\, ω(z'') + ω(Hz'') + F'(z') &\pmod{αβγ} & (2)\\
0 & (β + M_0)\, ω (x'') + ω(Hx'') + ω(z'') + F''(x', z'), &\pmod{α} & (3)\\
\end{array}\right.
\end{equation}
where the various~$F()$ are constants depending only on the (known)
values of~$x'$ and~$z'$. Let~$x''$ be any solution of the first equation.
The remaining equations on~$z''$ are then of the form
\begin{equation}\label{eq:system-z}
\left\{ \begin{array}{c@{\;≡\;}ll@{\qquad}c}
φ(z'') & ω(Hz'') + C &\pmod{αβγ} & (2)\\
ω(z'') & C', &\pmod{α} & (3)\\
\end{array}\right.
\end{equation}
with both constants~$C$, $C'$ depending on~$x$ and~$z'$. Let~$z'' = ∑ z_i
H^i$. The second equation determines all values~$z_{2i+1}$, while the
first one is of the form~$z_i^2 + z_{2i} = C_i$. For~$i = 0$, this is an
Artin-Schreier equation, and the existence of solutions depends only on
the (easily computed in advance) value~$C_0$; for all~$i ≥ 1$, this
equation determines all the values~$z_{2i}$. Therefore, it is possible to
compute all the coefficients of~$z''$ in linear time, and the
coefficients of~$(x, z)$ in quadratic time.

% Performing the line transformation
% \begin{equation}\label{eq:line-transform}
% P = \mat{ β_2 & 1 & 0 & 0 \\ 0 & 0 & β_4 & 1 \\ β'_3 β_4 & β'_3 & β_2 & 1},
% \end{equation}
% we see that \eqref{eq:matrix1}~implies the following equation, where~$X =
% \mat{x \\ y}$:
% \begin{equation}\label{eq:matrix2}
% α'_1 \mat{β_2^2 & 1 \\ β'_3 β_4^2 & β'_3 \\ 0 & 0} φ (X)
% + \mat{β_2 & 1 \\ 0 & 0 \\ β'_3 β_4 & β_3} θ(X)
% + \mat{0 & 0\\ β_4 & 1 \\ β_2 & 1} θ(MX) = C,
% \end{equation}
% where $C$~is a column vector. Let~$Y = \mat{φ^{-1}(β_2)^2 & 1 \\
% φ^{-1}(β_4) & 1} X$ and note that the relation~$θ(a^2 x) = φ(a) θ(x)$
% implies
% \begin{equation}
% φ(Y) = \mat{β_2^2 & 1 \\ β_4^2 & 1} φ(X), \quad
% θ(Y) = \mat{β_2 & 1 \\ β_4 & 1} θ(X).
% \end{equation}
% Since all matrices in~\eqref{eq:matrix2} factor to the right as
% appropriate, that equation is equivalent to
% \begin{equation}\label{eq:matrix3}
% α'_1 \mat{1 & 0 \\ 0 & β'_3 \\ 0 & 0} φ(Y)
% + \mat{1 & 0 \\ 0 & 0 \\ 0 & β_3} θ(Y)
% + \mat{0 & 0 \\ 0 & 1 \\ 1 & 0} θ(MY) = C.
% \end{equation}
% If $β_2 - β_4$~is invertible in~$R$, then this last equation is
% equivalent to~\eqref{eq:matrix1}; in the general case,
% \eqref{eq:matrix1}~is equivalent to~\eqref{eq:matrix2} plus the extra
% equation
% \begin{equation}\label{eq:matrix-x}
% (α'_1 α_4 + α_2 α'_3) φ(x) + β'_3 θ(x) + θ(Mx) = 0.
% \end{equation}
%>>>

\end{document}
