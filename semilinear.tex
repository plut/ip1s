\documentclass{llncs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{unicode}
\usepackage{amsfonts}% 
\usepackage{amssymb}% 
\usepackage{amsmath}% 
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\def\mat#1{\begin{pmatrix}#1\end{pmatrix}}

\def\abs#1{\left|#1\right|}

\begin{document}
\title{Semi-linear equations appearing in the IP1S problem}

\section{Equations of the IP1S problem}
Let~$k$ be a finite field of characteristic two and~$R$ be the local
$k$-algebra $R = k[H]/H^d$. The equations for the (cyclic) IP1S problem
are given by
\begin{gather*}\label{eq:matrix1}
u α'_1 = α_1 + α_2 φ(x) + θ(x),\\
\numberthis
u α_2 = α'_2 + α'_1 φ(y) + θ(y),\\
u α'_3 = α_3 + α_4 φ(x) + θ(M\,x),\\
u α_4 = α'_4 + α'_3 φ(y) + θ(M\,y).\\
\end{gather*}
Without changing the values~$α_1, α_2$, we may further assume
the divisibility properties: $α'_1 \mid α_2$, $α'_1 \mid
α'_3 \mid α_4$. Let~$β_i ∈ k[H]$ be such that
\begin{equation}\label{eq:beta}
α_2 = α'_1 β_2, \qquad
α_3 = α'_1 β'_3, \qquad
α_4 = α'_3 β_4 = α'_1 β'_3 β_4.
\end{equation}
When the matrices are in reduced form, the first equation
of~\eqref{eq:matrix1} always determines an invertible~$u ∈ R$. Therefore,
we may eliminate~$u$ by performing an elementary line transformation.

We choose the line transformation
\begin{equation}\label{eq:line-transform}
P = \mat{ β_2 & 1 & 0 & 0 \\ 0 & β_3 & 0 & 1 \\ β'_3 β_4 & β'_3 & β_2 & 1},
\end{equation}
which gives the equations
\begin{equation}\label{eq:matrix2}
α'_1 \mat{β_2^2 & 1 \\ β'_3 (β_2 + β_4) & 0 \\ 0 & 0} φ (X)
+ \mat{β_2 & 1 \\ β'_3 & 0 \\ β'_3 β_4 & β_3} θ (X)
+ \mat{0 & 0 \\ 1 & 0 \\ β_2 & 1} θ (MX) =
\mat{α_1 β_2 + α'_2 \\ α_1 β'_3 + α_3 \\
α_1 β'_3 β_4 + α'_2 β'_3 + α_3 β_2 + α'_4}.
\end{equation}
Let~$Y = \mat{1 & 0 \\ φ^{-1}(β_2)^2 & 1} X$ and note that the
relation~$θ(a^2 x) = φ(a) θ(x)$ implies
\begin{equation}
φ(Y) = \mat{1 & 0 \\ β_2^2 & 1} φ(X), \quad
θ(Y) = \mat{1 & 0 \\ β_2 & 1} θ(X).
\end{equation}
Factoring on the right the matrices from equation~\eqref{eq:matrix2}, we
obtain the following:
\begin{equation}\label{eq:matrix3}
α'_1 \mat{0 & 1 \\β'_3 (β_2 + β_4) & 0 \\ 0 & 0} φ(Y)
+ \mat{0 & 1 \\ β'_3 & 0 \\ β'_3(β_2 + β_4) & β_3} θ(Y)
+ \mat{0 & 0 \\ 1 & 0 \\ 0 & 1} θ(MX) = C.
\end{equation}

% Performing the line transformation
% \begin{equation}\label{eq:line-transform}
% P = \mat{ β_2 & 1 & 0 & 0 \\ 0 & 0 & β_4 & 1 \\ β'_3 β_4 & β'_3 & β_2 & 1},
% \end{equation}
% we see that \eqref{eq:matrix1}~implies the following equation, where~$X =
% \mat{x \\ y}$:
% \begin{equation}\label{eq:matrix2}
% α'_1 \mat{β_2^2 & 1 \\ β'_3 β_4^2 & β'_3 \\ 0 & 0} φ (X)
% + \mat{β_2 & 1 \\ 0 & 0 \\ β'_3 β_4 & β_3} θ(X)
% + \mat{0 & 0\\ β_4 & 1 \\ β_2 & 1} θ(MX) = C,
% \end{equation}
% where $C$~is a column vector. Let~$Y = \mat{φ^{-1}(β_2)^2 & 1 \\
% φ^{-1}(β_4) & 1} X$ and note that the relation~$θ(a^2 x) = φ(a) θ(x)$
% implies
% \begin{equation}
% φ(Y) = \mat{β_2^2 & 1 \\ β_4^2 & 1} φ(X), \quad
% θ(Y) = \mat{β_2 & 1 \\ β_4 & 1} θ(X).
% \end{equation}
% Since all matrices in~\eqref{eq:matrix2} factor to the right as
% appropriate, that equation is equivalent to
% \begin{equation}\label{eq:matrix3}
% α'_1 \mat{1 & 0 \\ 0 & β'_3 \\ 0 & 0} φ(Y)
% + \mat{1 & 0 \\ 0 & 0 \\ 0 & β_3} θ(Y)
% + \mat{0 & 0 \\ 0 & 1 \\ 1 & 0} θ(MY) = C.
% \end{equation}
% If $β_2 - β_4$~is invertible in~$R$, then this last equation is
% equivalent to~\eqref{eq:matrix1}; in the general case,
% \eqref{eq:matrix1}~is equivalent to~\eqref{eq:matrix2} plus the extra
% equation
% \begin{equation}\label{eq:matrix-x}
% (α'_1 α_4 + α_2 α'_3) φ(x) + β'_3 θ(x) + θ(Mx) = 0.
% \end{equation}


\section{Solving semi-linear equations in local algebras}
We define two $k$-linear endomorphisms of~$R$
by
\begin{gather*}
θ(∑ x_i H^i) \;=\; ∑_{2i ≥ (d-1)} x_{2i-(d-1)} H^i,\\
ω(∑ x_i H^i) \;=\; ∑_{2i ≤ d)} x_{2i+1} H^{i}.
\end{gather*}
and extend the absolute Frobenius automorphism of~$k$ to~$R$ by
\begin{equation}
φ(∑ x_i H^i) \;=\; ∑ x_{i}^2 H^i.
\end{equation}
We also define the \emph{pseudo-inverse} of a non-zero element~$a = a'
H^r$ of~$R$, where $a'$~is invertible, by
\begin{equation}\label{eq:pseudo-inv}
\widehat{a} \;=\; \frac{1}{a'} H^{d-r}.
\end{equation}
The pseudo-inverse of~$a$ generates the annulator ideal of~$a$ in~$R$.
We note that we have the following relations between~$θ$, $ω$ and~$φ$:
\begin{gather*}\label{eq:theta-omega}
φ(a) θ(x) = θ(a^2 x); \quad
φ(a) ω(x) = ω(a^2 x); \quad
θ(H^{d-2r} x) = H^{d-r} ω(x);\\
a \widehat{a} = 0; \quad a \widehat{a^2} = \widehat{a};\quad
φ(\widehat{a}) = \widehat{φ(a)};\quad
θ(\widehat{a^2} x) = \widehat{φ(a)} ω(x).
\end{gather*}


\begin{proposition}\label{prop:contracting}
For any~$b, c, d ∈ R$, the equation
\begin{equation}
φ(x) = b θ(x) + c θ(Hx) + d
\end{equation}
has:
\begin{itemize}
\item zero or two solutions in~$R$ if $b$~is invertible;
\item one solution in~$R$ if $b$~is not invertible.
\end{itemize}
\end{proposition}


\begin{proof}
The map~$x ↦ a b(x) + b \pmod{H^{d-1}}$ is contracting for the $H$-adic
valuation on~$R$. Therefore, the equation has a unique solution
modulo~$H^{d-1}$. Solving for the coefficient of~$H^{d-1}$ yields:
\begin{itemize}
\item if $a$~is invertible, an Artin-Schreier equation with either zero or
two solutions (depending on the trace of~$b_{d-1}$);
\item if $a$~is not invertible, a pseudo-linear equation with exactly one
solution.
\end{itemize}
\end{proof}




\begin{proposition}\label{prop:eq}
XXX We can solve the equation
\begin{equation}\label{eq:theta}
a φ(x) = b θ(x) + c θ(Hx) + ad
\end{equation}
\end{proposition}

\begin{proof}
Assume that $x$~is a solution of~\eqref{eq:theta}. Then $y = φ^{-1}(a^2)
x$ satisfies~$φ(y) = a^2 φ(x)$ and~$θ(y) = a θ(x)$, and therefore
\begin{equation}\label{eq:theta-y}
φ(y) ≡ b θ(y) + c θ(Hy) + a^2d \pmod{\widehat{a}}.
\end{equation}
Conversely, let~$y_0$~be a solution of~\eqref{eq:theta-y}. Since~$v(y) ≥
2 v(a)$, we may write~$y_0 = φ^{-1}(a^2) x_0$ with~$x_0 ∈ R$; this is
then a solution of
\begin{equation}\label{eq:theta-x0}
a φ(x_0) ≡ b θ(x_0) + c θ(H x_0) + d - \widehat{a} d_1,\quad
\text{for some~$d_1 ∈ R$.}
\end{equation}
Write~$x = x_0 + \widehat{φ^{-1}(a^2)} x_1$. Equation~\eqref{eq:theta} is
then equivalent to
\begin{equation}\label{eq:omega-x1}
φ(x_1) = b ω(x_1) + c ω(H x_1) + d_1 \pmod{a}.
\end{equation}
\end{proof}

% \begin{proposition}\label{prop:omega}
% For any~$b ∈ R$ and integers~$r ≥ s$, the equation
% \begin{equation}
% φ(x) ≡ ω(H^s x) + b \pmod{H^{r - s}}
% \end{equation}
% has exactly~$\abs{k}^r$ solutions in~$k[H]/H^{2r-s}$.
% \end{proposition}
% 
% 
% \begin{proof}
% The equation reads as
% \begin{equation}
% x_{i}^2 = x_{2i+s+1} + b_i, \qquad \text{for~$i = 0, …, r-s-1$}.
% \end{equation}
% As~$i < 2i+s+1$ for all~$i$, this system is triangular with~$(2r-s)$
% variables and~$(r-s)$ equations.
% \end{proof}
% 
% 
% \begin{proposition}\label{prop:theta}
% For all integers~$r ≤ (d-1)/2$ and~$b ∈ R$ such that~$v_H(b) ≥ r$,
% the equation
% \begin{equation}\label{eq:theta}
% H^r φ(x) = a θ(x) + b.
% \end{equation}
% has either zero or $2·\abs{k}^{r}$ solutions in~$R$ if $a$~is invertible,
% and $\abs{k}^r$ solutions if $a$~is not invertible.
% \end{proposition}
% 
% \begin{proof}
% For any solution~$x$ of~\eqref{eq:theta}, it is readily seen that~$y =
% H^{2r} x$ is a solution of
% \begin{equation}\label{eq:theta-y}
% φ(y) = a θ(y) + H^r b.
% \end{equation}
% Conversely, assume that~\eqref{eq:theta-y} has a solution~$y_0 ∈ R$.
% As~$v_H(H^r b) ≥ 2r$, $y_0$~is divisible by~$H^{2r}$ in~$R$, so that we
% may write~$y_0 = H^{2r} x_0$. We then have
% \begin{equation}
% H^{2r} φ(x_0) = a θ(H^{2r} x_0) + b = H^r (a θ(x_0) + b).
% \end{equation}
% From this we deduce that~$H^r φ(x_0) ≡ a θ(x_0) + b \pmod{H^{d-r}}$.
% Let~$b_1 ∈ R$ such that~$H^r φ(x_0) = a θ(x_0) + b + H^{d-r} b_1$, and
% write~$x = x_0 + H^{d-2r} x_1$; using relation~\ref{eq:theta-omega},
% equation~\ref{eq:theta} is then equivalent to
% \begin{equation}\label{eq:theta1}
% φ (x_1) ≡ a ω(x_1) + b_1 \pmod{H^r}, \quad x_1 ∈ k[H]/(H^{2r}).
% \end{equation}
% Write~$a = H^s a'$ with $a'$~invertible in~$R$, and let~$x_1 =
% φ^{-1}(a')^2 x_2 + φ^{-1} (b_1)$ and~$b_2 = θ ∘ φ^{-1}(b_1/a')$; then
% \eqref{eq:theta1}~is in turn equivalent to
% \begin{equation}\label{eq:theta2}
% φ (x_2) ≡ H^s (ω(x_2) + b_2) \pmod{H^r}, \quad x_2 ∈ k[H]/(H^{2r}).
% \end{equation}
% This obviously implies that~$x_2 ∈ H^s R$. Let~$x_2 = H^s x_3$.
% Equation~\eqref{eq:theta2} is then equivalent to
% \begin{equation}\label{eq:theta3}
% φ (x_3) ≡ ω(H^s x_3) + b_3 \pmod{H^{r-s}},
%   \quad x_3 ∈ k[H]/(H^{2r-s}).
% \end{equation}
% Applying Prop.~\ref{prop:omega} gives the result.
% \end{proof}
% 

\end{document}
