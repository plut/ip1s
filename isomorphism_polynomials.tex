\documentclass{lms}%<<<1
%\usepackage[margin=30mm]{geometry}
%\usepackage[letterpaper,hmargin=1.4in,vmargin=1.4in]{geometry}
\usepackage{unicode}
\DeclareUnicodeCharacter{22F1}{\smash\ddots}
\DeclareUnicodeCharacter{22F0}{\smash\iddots}
\usepackage{amsmath,amssymb,mathrsfs}
\usepackage{bm}
\usepackage{mathdots}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newnumbered{definition}{Definition}

% Theorems, equations, counters%<<<
\def\linkcounter#1#2{\edef\magic{\noexpand\let
  \expandafter\noexpand\csname c@#1\endcsname
  \expandafter\noexpand\csname c@#2\endcsname}\magic}
\def\newthm#1#2{\newtheorem{#1}{#2}[section]\linkcounter{#1}{equation}}
\newthm{prop}{Proposition}
\newthm{thm}{Theorem}
\newthm{lem}{Lemma}

\def\labelenumi{(\roman{enumi})} \let\itemref\ref
%>>>
% Misc. math stuff%<<<
\def\bigperp{\mathop{\vcenter{\hbox{\scalebox{2}{\ensuremath{\perp}}}}}%
  \displaylimits}
\let\fr\mathfrak
\let\ro\mathscr
\def\transpose{\,{}^{\mathrm{t}\!}}
\def\pa#1{\left(\right)}
\def\acco#1{\left\{#1\right\}}
\def\abs#1{\left|#1\right|}
\def\mat#1{\begin{pmatrix}#1\end{pmatrix}}
\def\smat{\def\arraystretch{.66}\mat}
\def\card#1{\abs{#1}}
\DeclareMathOperator\Ker{Ker}
\DeclareMathOperator\End{End}
\DeclareMathOperator\GL{GL}
\def\F{\mathbb{F}}
\def\Id{\mathrm{Id}}
\def\Ot{\widetilde{O}}
%>>>
\usepackage{color}
\def\commentaire#1{{\bfseries\textcolor{red}{#1}}}
%>>>1

\begin{document}
\title[Solving ``Isomorphism of Polynomials with Two Secrets'']%
{Solving the ``Isomorphism of Polynomials with Two Secrets'' Problem}%<<<1
%\author{Pierre-Alain Fouque \and Gilles Macario-Rat \and Jérôme Plût}
\author{Submission to ANTS 2014}
\maketitle

%\commentaire{deuxième essai avec mise en ligne du PDF}
\begin{abstract}
In this paper, we study the Isomorphism of Polynomial (IP) problem
with~$m=2$ homogeneous quadratic polynomials of $n$ variables over a finite field of odd
characteristic: given two quadratic polynomials $(\bm{a},\bm{b})$ 
on $n$ variables, we find two bijective linear maps $(s,t)$ such that
$\bm{b}=t\circ \bm{a}\circ s$. We give an algorithm computing~$s$ and~$t$
in time complexity~$\Ot(n^4)$.
This problem has been introduced in cryptography by Patarin back in 1996.
One special instance of this problem when $t$ is the identity is called
the isomorphism with one secret (IP1S) problem.
Generic algebraic equation solvers (for example using Gröbner bases)
solve quite well random instances of the IP1S problem. For the particlar
\emph{cyclic} instances of IP1S, a cubic-time algorithm was later
given~\cite{MPG2013} and explained in terms of pencils of quadratic forms
over all finite fields; in particular, the cyclic IP1S problem in odd
characteristic reduces to the computation of the square root of a matrix.

We give here an algorithm solving all cases of the IP1S problem in odd
characteristic using two new tools, the Kronecker form for a singular
quadratic pencils, and reduction bilinear forms over a non-commutative
algebra. Finally, we show that the second secret in the IP2S problem may
be recovered in cubic time.
\end{abstract}

\section{Introduction}
The \emph{Isomorphism of Polynomial with Two Secrets} (IP2S) problem is
the following. Given a field~$k$ and two $m$-uples $\bm{a} = (a_1, …,
a_m)$ and~$\bm{b} = (b_1, …, b_m)$ in $n$~variables~$(x_1, …, x_m)$, compute
two invertible linear maps~$s ∈ \GL_n(k)$ of the variables~$x_i$
and~$t ∈ \GL_m(k)$ of the polynomials~$a_i$ such that
\begin{equation*}
b = t ∘ a ∘ s.
\end{equation*}
The particular case where we restrict~$t$ to the identity transformation
is also known as the \emph{Isomorphism of Polynomials with One Secret}
(IP1S). Both these problems have been introduced in cryptography by
Patarin in~\cite{DBLP:conf/eurocrypt/Patarin96} to construct an efficient
authentication scheme, 
as an alternative to the Graph Isomorphism Problem (GI) proposed by Goldreich, Micali and Wigderson~\cite{DBLP:journals/jacm/GoldreichMW91}.
The IP problem was appealing since 
it seems more difficult than the Graph Isomorphism problem~\cite{DBLP:conf/eurocrypt/PatarinGC98}. 
Agrawal and Saxena reduced~\cite{DBLP:conf/stacs/AgrawalS06} the Graph
Isomorphism problem to the particular case of IP1S using two polynomials,
one of them being a quadratic form encoding the adjacency matrix of the
graph, and the other one being the cubic~$\sum x_i^3$, over a finite field of
odd characteristic.
For the case of quadratic polynomials, the status of this problem is unclear despite recent 
intensive research in the cryptographic community since this case is the most interesting 
for practical schemes. There exists a claimed reduction between the quadratic IP1S problem and the GI
problem~\cite{DBLP:conf/eurocrypt/PatarinGC98}, but we realized that this proof is incomplete. 
Indeed, the proof works by induction and decompose any permutation as the composition of 
transpositions. It is possible to write a system of quadratic polynomials such that the only solutions 
of the IP1S problem will be the identity or a transposition by modifying a bit the systems proposed 
in~\cite{DBLP:conf/eurocrypt/PatarinGC98}. However, it is not obvious how we can compose the systems 
of equations such that the solutions will be the composition of the solutions. 


The defining parameters of the IP problems are the number~$n$ of
variables, the number~$m$ of polynomials, and their degree. For
efficiency reasons, the degree is generally small, involving only
quadratic and cubic equations. To our knowledge, no significant progress
has been done on the cubic case.

We limit ourselves to the special case of two equations, both of which
being homogeneous polynomials of degree~two. According to previous
literature~\cite{DBLP:conf/eurocrypt/Perret05,DBLP:conf/eurocrypt/FaugereP06,DBLP:conf/pkc/BouillaguetFFP11,DBLP:conf/eurocrypt/BouillaguetFV13},
this is the most difficult case.

The case with only one homogeneous quadratic equation amounts to
reduction of quadratic forms, which has been known for
centuries~\cite{gauss,lidl1997finite}. In the non-homogeneous case, the
presence of affine terms gives linear relations between the secret
unknowns~\cite{DBLP:conf/eurocrypt/PatarinGC98}, and this extra
information actually helps generic solvers, for example those using
Gröbner bases, as shown in~\cite{DBLP:conf/eurocrypt/FaugereP06}.
The case with more than two equations is easier since we can relinearize
the systems~\cite{DBLP:conf/pkc/BouillaguetFFP11}.

\paragraph{Previous Work.}

Some recent advances have been made on the IP1S problem in the case of
two
homogeneous quadratic equations.


Bouillaguet, Fouque and Macario-Rat in 2011~\cite{DBLP:conf/asiacrypt/BouillaguetFM11} 
used pencil of quadratic forms to recover the secret mappings $s$ and $t$
when three equations are available
and one of the quadratic equations $\bm{a}$ comes from a special mapping $X\mapsto X^{q^\theta+1}$ 
over $\F_q$. In the case of the IP problem, this is optimal using an information theoretic argument.

Macario-Rat, Plût and Gilbert in 2013~\cite{MPG2013} how to solve cyclic
instances of the IP1S problem for~$m=2$ over finite fields of any
characteristic. Cyclic instances are instances~$\bm{b} = (b_{∞}, b_{0})$
such that $b_{∞}$~is invertible and~$b_{∞}^{-1} b_{0}$ is cyclic.
Although the cyclic case is generic in the geometric sense (\emph{i.e.}
defined by the non-cancellation of some polynomial functions of the
coefficients of~$\bm{b}$), it is not the general case in a practical
sense. In the cyclic instances, Gröbner basis works well~\cite{DBLP:conf/pkc/BouillaguetFFP11} 
since in this case, the number of solutions is small. For all other instances, the number of 
solutions is large, and it is well-known that in this case, such algorithms are less efficient. 

%\commentaire{2. parler du travail de Jeremy + Faugere et Ludovic}
Finally, very recently Berthomieu, Faugère and Perret in~\cite{DBLP:journals/corr/BerthomieuFP13}
proposed a polynomial algorithm solving the \emph{decisional} IP1S
problem for any number of equations. The decisional problem requires to
determine whether an isomorphism exists between two given families of
polynomials; this problem does not have many cryptographic applications.

%However, some inconsistancies are present in this work and mismatches 
%come from some parts of their algorithms work in zero characteristic fields while other parts are for 
%finite fields. First of all, 
%they solved the problem in an extension field of the base field. They turn the congruence problem, 
%\textit{i.e.} given two systems of matrices $\{H_i\}_{1\leq i \leq m}$ and $\{H'_i\}_{1\leq i \leq m}$, 
%find an invertible matrix $P$ such that $H'_i=\transpose{P}H_iP$ into a conjugaison problem, 
%\textit{i.e.} given two systems of matrices $\{H_i\}_{1\leq i \leq m}$ and $\{H'_i\}_{1\leq i \leq m}$, 
%find an invertible matrix $P$ such that $H'_i=P^{-1}H_iP$. Using a result proved in~\cite{dSP10} concerning 
%the conjugaison problem, they
%consider that it is equivalent to solve the problem in an extension field. However, there is cases where
%the congruence problem has a solution in the extension but not in the base field. They assume that 
%the two systems can always been transformed into two systems such that the first is the identity and 
%this is the case if we can always find a square root, but this is not always possible. 
Their deterministic algorithm works in in the field of real algebraic number or the field of algebraic number, 
since they use some results of~\cite{DBLP:conf/issac/ChistovIK97} which work in such fields.

\paragraph*{Our contributions.}%<<<2
We first reduce the general IP1S problem to the \emph{regular case},
which is the case where $b_{∞}$~is invertible. This is the object of
section~\ref{S:IP1S-singular} of this document and uses the Kronecker
classification of pencils of quadratic forms.

We then prove that the regular case of the IP1S problem simplifies to a
reduction problem for some quadratic forms over a local algebra. As this
is a well-understood theory (in odd characteristic), we are able to give
a polynomial-time answer to all instances of IP1S in
section~\ref{S:IP1S-regular}.

The last section explains how we recover the second (``outer'') secret in
the two-secret problem.
\commentaire{ ? la partie singulière n'intervient pas et donc tout
secret~$h$ possible sur la partie régulière est bon ?}

\subsection{Mathematical Background and notations}%<<<2
Let~$V$ be a $n$-dimensional vector space over the field~$k$. We study
the IP1S and IP2S problems for \emph{quadratic forms} on~$V$, which are
homogeneous polynomials of degree~$2$ in some coordinates on~$V$. To a
quadratic form~$q$, one may associate the \emph{polar form}~$b$ defined
by
\begin{equation*}\label{eq:polar}
b(x,y) = q(x+y) - q(x) - q(y);
\end{equation*}
this is a symmetric bilinear forms, and it satisfies the \emph{polarity
identity}
\begin{equation*}\label{eq:polarity}
b(x,x) = 2q(x).
\end{equation*}
If~$2 ≠ 0$ in~$k$, then the polarity identity is a bijection between
quadratic forms and bilinear forms. Therefore, instead of quadratic
forms, we shall study directly bilinear forms.

We note that, if~$2 = 0$ in~$k$, the situation is much more complicated;
the polarity identity is no longer a bijection, but polar forms are
instead alternate bilinear forms. This means that their classification is
very different from the odd-characteristic
case~\cite{milnorhusemoller}, and relies on symplectic groups and
Artin-Schreier type equations, \emph{i.e.} of the type~$x^2+x+C = 0$.

Let~$V^{∨}$ be the dual of the vector space~$V$. A bilinear form~$b$
on~$V$ is the same as a linear map~$b: V → V^{∨}$. The bilinear form is
\emph{regular} if it defines an invertible linear map~$V → V^{∨}$. In
this case, to for any endomorphism~$u$ of~$V$, there exists a unique
endomorphism~$u^{⋆}$ of~$V$ such that~$b(x,u(y)) = b(u^{⋆}(x), y)$; the
endomorphism~$u^{⋆}$ is called the \emph{left-adjoint} of~$u$. If $b$~is
symmetric then left- and right-adjoints coincide.

A bilinear form~$b$ on~$V$ is \emph{regular} if the associated We
write~$\transpose{A}$ for the transpose of a matrix~$A$. A
\emph{symmetric} matrix is a matrix such that~$A = \transpose{A}$.
Symmetric bilinear forms~$b$ correspond to symmetric matrices~$B$.
A bilinear form is regular iff its matrix is invertible. For any
endomorphism~$u$ with matrix~$U$, the adjoint endomorphism has
matrix~$U^{⋆} = B^{-1} · \transpose{U} · B$.

% \begin{definition}%\textbf{[Quadratic IP1S]}% \\
% The homogeneous quadratic IP1S problem is the following. Given two
% $m$-uples $\bm{a} = (a_1, \ldots, a_{m})$ and {$\bm{b} = (b_1, \ldots,
% b_{m})$}  of quadratic forms in $n$ variables over $k$, find a
% non-singular linear mapping $s \in \mathrm{GL}_n(k)$ (if any) such that
% $\bm{b} = \bm{a} \circ s$, i.e. $a_i = b_i \circ s$ for $i= 1, \ldots,
% m$.
% \end{definition}


%>>>1
\section{IP1S in the possibly singular case}%<<<1
\label{S:IP1S-singular}
\subsection{Regular and singular pencils}%<<<2
Throughout this document, $k$~is a field such that~$2 ∈ k^{×}$. Let~$V$
be a vector space. An \emph{(affine) pencil of symmetric bilinear forms}
over~$V$, or a \emph{symmetric pencil} in short, is the data of a pair of
symmetric bilinear forms~$\bm{b} = (b_{∞}, b_{0})$ over~$V$. We write
this pencil in affine form as~$b_{λ} = -λ b_{∞} + b_{0}$, and in
projective form as~$b_{λ:μ} = -λ b_{∞} + μ b_{0}$, where $b_{0}$
and~$b_{∞}$ are symmetric bilinear forms.

The \emph{characteristic polynomial} of the symmetric pencil~$(b_{λ})$ is
either the polynomial~$f(λ) = \det (λ b_{∞} + b_{0})$, or its homogeneous
form~$f(λ: μ) = \det (λ b_{∞} + μ b_{0})$. If $\dim_{k} V = n$, then
$f(λ: μ)$~is homogeneous of degree~$n$. The pencil~$(b_{λ})$ is called
\emph{regular} if the characteristic polynomial is not zero, and
\emph{singular} otherwise. We solve the isomorphism problem for regular
pencils in section~\ref{S:IP1S-regular} below.

We reduce to the regular case by proving that the singular part of a
symmetric pencil is reducible to the canonical form of Kronecker. This
form is described in~\cite[XII(56)]{Gantmacher2}; however, the proof
given there only applies to pencils over~$ℂ$, as its uses the computation
of square roots of matrices via interpolation on the spectrum. We give
here a proof that is true over any field~$k$ such that~$2 ≠ 0$ and which
is algorithmic.

\subsection{Reduction of (possibly singular) pencils to the Kronecker form} %<<<2
% Définition: indice minimal%<<<
The pencil~$(b_{λ})$ defines a symmetric bilinear form on the
module~$V_{∞} = V ⊗_{k} k[λ]$; if $(b_{λ})$~is singular, then this form
has a non-trivial kernel~$W$. Elements of~$W$ are called \emph{isotropic}
for~$(b_{λ})$. An element~$x = x_0 + λ x_1 + … + λ^h x_h$ is isotropic iff
\begin{equation}\label{eq:isotropic}
b_0 x_0 = 0, \quad
b_0 x_1 = b_{∞} x_0, \quad …
b_0 x_h = b_{∞} x_{h-1}, \quad
b_{∞} x_{h} = 0.
\end{equation}
A \emph{minimal isotropic vector} for~$(b_{λ})$ is one with minimal
degree~$h$; this degree is the \emph{minimal index} of~$(b_{λ})$. If
$(b_{λ})$~is regular, then the minimal index is~$+∞$.
By choosing a basis of~$W$
adapted to the filtration of~$V_{∞}$ by the degree of polynomials, we see
that $W$~as a basis~$(w_1,…,w_r)$ such that, if $h_i$~is the degree of
the isotropic vectol~$w_i$, then $(w_i,…,w_r)$~generate no isotropic
vector of degree~$< h_i$. The degree~$h_i$ are called the \emph{minimal
indices} of the pencil~$(b_{λ})$.
%>>>
\begin{prop}\label{prop:minimal-indep}%<<<
Let~$e = ∑ λ^i e_i$ be a minimal isotropic vector for~$(b_{λ})$. Then
\begin{enumerate}
\item The~$h+1$ vectors~$e_0, …, e_h$ are $k$-linearly independent.
\item The~$h$ linear forms~$b_{0} e_1, …, b_{0} e_h$ are $k$-linearly
independent.
\end{enumerate}
\end{prop}

\begin{proof}
We first prove~(ii). Assume that there exists a non-trivial linear
relation~$α_1 e_0 + … + α_h e_{i}$ satisfies the relations
\begin{equation}\label{eq:relation-e'}
\begin{split}
b_0 e'_0 &= α_h b_0 e_0 = 0, \\
b_0 e'_i &= α_{h-i+1} b_0 e_1 + … + α_m b_0 e_{i-1} = b_{∞} e'_{i-1}, \\
b_{∞} e'_{h-1} &= b_{∞} (α_1 e_{0} + … + α_{h} e_{h-1}) \\
 &= b_{0} (α_1 e_{1} + … + α_{h} e_{h}) \\
 &= 0.
\end{split}
\end{equation}
This means that~$(e'_0+…+λ^{h-1} e'_{h-1})$ is isotropic and of degree~$≤
h-1$ for~$b_{λ}$, which contradicts the minimality of~$e$.

To prove~(i), let~$α_0 e_0 + … + α_h e_h = 0$ be a non-trivial linear
relation. Then since $α_1 b_0(e_1) + … + α_h b_0(e_h) = 0$, by~(ii) we
must have~$α_1 = … = α_h = 0$, which in turn implies~$e_0 = 0$. However,
in this case we see that~$e_1+…+ λ^{h-1} e_h$ is isotropic of degree~$≤
h-1$.
\end{proof}
%>>>

Define matrices~$K'_0, K'_{∞}$ of size~$(h+1)×d$ and $K_{0}, K_{∞}$ of
size~$(2h+1)×(2h+1)$ by
\begin{equation}\label{eq:def-K}
K'_0 = \mat{0&&0\\1&⋱&\\&⋱&0\\0&&1}, \quad
K'_{∞} = \mat{1&&0\\0&⋱&\\&⋱&1\\0&&0}, \qquad
K_{λ} = \mat{0&K'_{λ}\\\transpose{K'_{λ}}&0}.
\end{equation}

\begin{prop}\label{prop:kronecker1}%<<<
Let~$(b_{λ})$ be a symmetric pencil with minimal index~$h$. There exists
a basis of~$V$ in which the pencil~$(b_{λ})$ has the matrix 
\[ B_{λ} = \mat{K_λ&0\\0&B'_{λ}}, \]
where $B'_{λ}$ is a symmetric pencil with minimal index at least~$h$.
\end{prop}

Note in particular that the case~$h = 0$ corresponds to the matrix~$K_0$,
which is the zero matrix of size~$1 × 1$, and to a vector belonging to
all the kernels of~$b_{λ}$.

\begin{proof}
Let~$(e_0, …, e_h)$ be a minimal isotropic vector. By
Prop.~\ref{prop:minimal-indep}, there exist vectors~$f_1,…,f_h$ such
that~$b_0(e_i, f_j) = 1$ when~$i = j$ and~$0$ otherwise, and the
family~$(e_0,…, e_h, f_1, …, f_h)$ is free. In any basis completing this
family, the pencil~$(b_{λ})$ has the symmetric matrix
\begin{equation}\label{eq:matrix-b1}
B_{λ} = \mat{0 & K'_{λ} & 0\\\transpose{K'_{λ}} & A_{λ}&\transpose{C_{λ}}\\
  0 & C_{λ} & B'_{λ}},
\end{equation}
where the blocks have size~$d+1$, $d$ and~$n-(2d+1)$.
We first prove that we may assume that~$C_{λ} = 0$, using a change of
coordinates of the form
\begin{equation}\label{eq:p1}
P = \mat{1 & 0 & X\\0&1&0\\0&Y&1}.
\end{equation}
The action of~$P$ on the sub-matrix~$C_{λ}$ of~$B_{λ}$ is given by $C_{λ}
← C_{λ} + \transpose{X} K'_{λ} + B_{λ} Y$. Now let~$x_0,…,x_{h};\;
y_1,…,y_h;\; c_1,…,c_h;\; c'_1,…,c'_h$ be the columns of~$\transpose{X}, Y,
C_0$ and~$C_{∞}$. We then have to
solve the equations
\begin{equation}\label{eq:chvar-x}
\begin{cases} c'_i + x_i + B'_0 y_i = 0\\ i = 1,…,h\end{cases};\quad
\begin{cases} c'_i + x_{i-1} + B'_∞ y_i = 0\\i=1,…,h\end{cases}.
\end{equation}
This determines uniquely the values~$x_0$ and~$x_{h}$. The equations
for~$x_1,…,x_{h-1}$ have solutions iff the values~$y_1,…,y_h$ satisfy
the relations
$B'_0 y_i - B'_{∞} y_{i+1} = c'_{i+1} - c_i$ for~$i = 1,…,h-1$. This
translates into matrix form as
\begin{equation}\label{eq:chvar-y}
\mat{B'_0&-B'_{∞}&&0\\&⋱&⋱&\\0&&B'_0&-B'_{∞}} ·
  \mat{y_1\\⋮\\y_h} = \mat{c'_2-c_1\\⋮\\c'_h-c_{h-1}}.
\end{equation}
Let~$\ro B$ be the left-side matrix. Then any element of the kernel
of~$\transpose{\ro B}$ defines an isotropic vector of degree~$≤ h-1$
for~$(b_{λ})$~\cite[XII(10)]{Gantmacher2}.
This proves that $\transpose{\ro B}$~is injective and hence that $\ro
B$~is surjective. Therefore, equations~\eqref{eq:chvar-y}
and~\eqref{eq:chvar-x} have solutions.

We now prove that we may also assume~$A_{λ} = 0$ in~\eqref{eq:matrix-b1}.
The action of a coordinate change~$Q$ of the form
\begin{equation}\label{eq:chvar-q}
\def\arraystretch{.7}
Q = \mat{1&Z&\\&1\\&&1}
\end{equation}
on~$A_{λ}$ is given by~$A_{λ} ← A_{λ} + \transpose{Z} K_{λ} +
\transpose{K_{λ}} Z$. Let~$Z = (z_{i,j})$, $A_0 = (a_{i,j})$ and~$A_{∞} =
a'_{i,j}$; the equations to solve are then
\begin{equation}
z_{i,j} + z_{j,i} = a_{i,j}, \quad z_{i-1,j} + z_{j-1,i} = a'_{i,j},
\quad \text{for~$i,j = 1,…,h$}.
\end{equation}
Since the matrices~$A_{0}$ and~$A_{∞}$ are symmetric, only the
equations for~$i ≥ j$ are relevant. We then check that, since~$2 ≠ 0$
in~$k$, this is a regular system of~$h(h+1)$ equations in the~$h(h+1)$
unknowns~$z_{i,j}$. Therefore, there exists a unique solution matrix~$Z$.
This concludes the proof of Prop.~\ref{prop:kronecker1}.
\end{proof}%>>>

\begin{prop}[(Kronecker form)]\label{prop:kronecker}%<<<
Let~$(b_{λ})$ be a symmetric pencil on~$V$. There exists a basis of~$V$
in which the pencil has a block-diagonal matrix with diagonal
blocks~$(K_{h_1}, …, K_{h_r}, B')$, where $K_h = K_{h,0} + λ K_{h,∞}$~is
the square matrix of size~$2h+1$ defined by equation~\eqref{eq:def-K},
the integers~$h_1 ≤ … ≤ h_r$ are the minimal indices of~$(b_{λ})$, and
$B'$~is the matrix of a regular pencil.
\end{prop}
%>>>
\section{IP1S for regular pencils}%<<<1
\label{S:IP1S-regular}

We give here an algorithm for solving the IP1S problem in the case of two
regular pencils. Assume that $\bm{b} = (b_{λ})$~is regular, and let~$f ≠ 0$
be its characteristic polynomial. Then, for any~$λ$ such that~$f(λ) ≠ 0$,
the bilinear form~$b_{λ}$ is regular.

\subsection{Localisation of the IP1S problem}%<<<2

\begin{lem}\label{lem:decomp-bezout}%<<<
Let~$\bm{b}$ be a regular symmetric pencil on the vector space~$V$. Let~$f(λ:
μ)$~be the homogeneous characteristic polynomial of~$S$, and let~$f = ∏
g_i$ be a factorisation of~$f$ in mutually coprime factors.

Then there exists a unique decomposition~$V = ⨁ V_i$ such that the
spaces~$V_i$ are pairwise orthogonal for all forms of~$\bm{b}$ and the
restriction~$S|_{V_i}$ has characteristic polynomial~$g_i$.
\end{lem}

\begin{proof}
Let~$V_{∞} = \Ker b_{∞}$ and $V'$ be the orthogonal of~$b_{∞}$ relatively
to the bilinear form~$b_{0}$. Then the decomposition~$V = V' ⊕ V_{∞}$ is
orthogonal for all forms~$b_{λ}$; replacing~$V$ by~$V'$, we may assume
that $b_{∞}$~is a regular bilinear form. This implies that $b_0$~has an
adjoint endomorphism~$m = b_{∞}^{-1} b_0$ such that~$b_0(x,y) = b_{∞}(x,
ay) = b_{∞}(ax, y)$; in particular, all elements of the algebra~$k[m]$
are self-adjoint with respect to~$b_{∞}$.

Let~$f(λ) = f(λ: 1)$ be the affine characteristic polynomial. It is
enough to prove the result for the decomposition~$f = gh$ where $g, h$
are mutually prime. Let~$u, v$ be polynomials such that~$ug + vh = 1$ and
$x, y ∈ V$ such that~$g(m)(x) = 0$ and~$h(m)(y) = 0$; we may then write
\begin{equation}\label{eq:bezout}
\begin{split}
b_{∞} (x, y) & = b_{∞} (x,\: u(m) g(m) y + v(m) h(m) y ) \\
&= b_{∞} (u(m) g(m) x, y) \,+\, b_{∞} (x, v(m) h(m) y) \\
&= 0.
\end{split}
\end{equation}
Since~$y' = ay$ also verifies~$h(m)(y') = 0$, equation~\eqref{eq:bezout}
also proves that~$b_{0}(x,y) = b_{∞}(x,y') = 0$, and hence~$x, y$ are
orthogonal for all forms~$b_{λ}$.
\end{proof}%>>>

The decomposition of~$V$ obtained by applying
Lemma~\ref{lem:decomp-bezout} to the full factorisation of~$f$
over~$k[x]$ is the \emph{primary decomposition} of the pencil~$(b_{λ})$.
The restriction of the pencil to each summand~$V_i$ has as its
characteristic polynomial a power of a prime polynomial; such a pencil is
called \emph{local}.

If two regular pencils~$\bm{b}, \bm{b}'$ are isomorphic (in the IP1S sense), then
they have the same characteristic polynomial, and computing an
isomorphism between~$\bm{b}$ and~$\bm{b}'$ is the same as computing it on each
factor of the primary decomposition. Therefore, in what follows, we shall
assume that both pencils are local.

\medskip

Note that when $k = \F_q$~is a finite field, it may happen that $λ^q μ -
λ μ^q$ divides~$f(λ:μ) ≠ 0$, so that~$f(λ) = 0$ for all~$λ ∈ ℙ^1(k)$. In
this case, although $(b_{λ})$~is a regular pencil, all forms~$b_{λ}$ are
degenerate. However, the decomposition given by
Lemma~\ref{lem:decomp-bezout} still applies, and all the local pencils
given by this decomposition contain at least one non-degenerate form,
namely~$b_{0}$ on~$V_{∞}$ and~$b_{∞}$ in all other cases.
Swapping~$b_{∞}$ and~$b_{0}$ when required, we may assume that all local
pencils are \emph{finite}, \emph{i.e.} that $b_{∞}$~is a regular bilinear
form.

\bigskip

Let~$b_{λ} = λ b_{∞} + b_0$ be a finite pencil. We may use the
characteristic endomorphism~$m = b_{∞}^{-1}b_0$ to write~$\bm{b}$ in the form
\begin{equation}\label{eq:adjoint}
b_{λ} = b_{∞}\;(λ + m).
\end{equation}
The image of~$\bm{b}$ by a linear change of variables~$s$ is then
\begin{equation}\label{eq:adjoint-change}
\transpose{s} · b_{λ} · s = 
  \transpose{s} · b_{∞} · s\; ( λ + s^{-1} · m · s).
\end{equation}
Let~$b'_{λ} = b'_{∞} (λ + m')$ be a pencil isomorphic to~$\bm{b}$; then there
exists a change of variables~$t$ such that~$t^{-1} · m' · t = m$, and the
pencil~$\transpose{t} · b'_{λ} · t$ is of the form $b'_{∞} (λ + m)$, so
that we may assume that~$m' = m$. Computing the isomorphism
between~$b_{λ}$ and~$b'_{λ}$ then amounts to computing~$s$ such that
\begin{equation}
\transpose{s} · b_{λ} · s = b'_{λ}, \qquad s · m = m · s.
\end{equation}

We define the \emph{symmetrizing
space}~$\ro S(m)$ and the \emph{commutant}~$\ro C(m)$ as
\begin{equation}\begin{split}
\ro S(m) &= \acco{\text{$b$ symmetric bilinear such that $bm$~is symmetric} },\\
\ro C(m) &= \acco{\text{$a ∈ \End V$ such that~$am = ma$}}.
\end{split}\end{equation}
The invertible elements of~$\ro C(m)$ form the \emph{commutant
group}~$\ro C(m)^{×}$.

\begin{prop}\label{prop:structure-sym}
Let~$m$ be an endomorphism of~$V$.
\begin{enumerate}
\item \label{it:sym-inv} The set~$\ro S(m)$ contains a regular bilinear
form.
\item \label{it:sym-comm} For any regular bilinear form~$t ∈ \ro S(m)$
and any endomorphism~$a$ of~$V$, the bilinear form~$ta$ belongs to~$\ro
S(m)$ if and only if $a$~is self-adjoint with respect to~$t$ and $a ∈ \ro
C(m)$.
\item Any finite pencil with characteristic endomorphism~$m$ is of the
form $b_{λ} = ta (λ+m)$ where~$a ∈ \ro C(m)^{×}$.
\item Let~$b_{λ} = ta(λ+m)$ be a finite pencil and~$s ∈ \ro C(m)$.
Let~$s^{⋆} = t^{-1} · \transpose{s} · t$ be the adjoint of~$s$
relatively to the bilinear form~$t$. Then
\begin{equation}
\transpose{s} · b_{λ} · s = t\: (s^{⋆} a s)\: (λ + m).
\end{equation}
% \item Let~$a ∈ \ro C(m)$. Then $\transpose{(t·a)} = t·a^{⋆}$. In
% particular, $t·a$~is symmetric if, and only if, $a = a^{⋆}$.
\end{enumerate}
\end{prop}

\begin{proof}
Point~\itemref{it:sym-inv} is explicitly proven in Prop.~\ref{prop:big-T}
below. Assuming that $a$~is self-adjoint with respect to~$t$,
point~\itemref{it:sym-comm} follows from
\begin{equation}
\transpose{(tam)} \;=\; \transpose{m}\, \transpose{(ta)}
 \;=\; \transpose{m}\,ta = tma;
\end{equation}
since $t$~is regular, it is cancellable in the resulting equation~$tma
= tam$.
\end{proof}

\begin{prop}\label{prop:IP1S-congruence}
The congruence problem for finite symmetric pencils is equivalent to the
following: given an endomorphism~$m$ of~$V$ and two invertible
self-adjoint matrices~$a, a' ∈ \ro C(m)$, compute a matrix~$x ∈ \ro
C(m)^{×}$ such that~$x^{⋆} a x = a'$.
\end{prop}

In the particular case where $m$~is cyclic, the commutant~$\ro C(m)$ is
the (commutative) polynomial algebra~$k[m]$. In this case, solving the
IP1S problem is straightforward~\cite{MPG2013}. The proof given here
specializes in the cyclic case to the proof of~\cite{MPG2013}. Namely, if
$m$~is cyclic, then Prop.~\ref{prop:IP1S-congruence} amounts to
equivalence of 1-dimensional quadratic forms over~$k[m]$, which is simply
a square root computation.

\subsection{The local IP1S problem in matrix form}%<<<2

Assume that $b_{λ}$~is a finite local pencil with characteristic
endomorphism~$m$ and characteristic polynomial~$p^d$, where $p$~is an
irreducible polynomial of degree~$e$. Let~$M_p$ be the companion matrix
of~$p$; then $k[M_p]$~is isomorphic to the extension field~$K =
k[μ]/(p(μ))$. For any matrix~$A = (a_{i,j})$ of size~$u × v$ with
coefficients in~$K$, let~$A^{♭}$ be the (``flattened'') matrix of
size~$eu × ev$ with blocks of size~$e × e$ given by~$a_{i,j}(M_p)$. We
have~$1^{♭} = 1$ and~$(AB)^{♭} = A^{♭} B^{♭}$ whenever the product
exists. We write~$A ↦ A^{♯}$ for the inverse map where it is defined.

For any integer~$u$, let~$H_u$ be the companion matrix of the
polynomial~$x^u$. Then there exists a basis of~$V$ in which the
matrix~$M$ of~$m$ is block-diagonal, with diagonal blocks~$M_{n_i} =
(H_{n_i} + μ)^{♭}$, for integers~$n_1 ≥ … ≥ n_r$.

\begin{lem}\label{lem:commute-jordan}%<<<
For all integers~$u, v$, define a matrix~$J_{u,v}$ of size~$u × v$ as
\begin{equation}
\def\arraystretch{.9}
J_{u,v} = \mat{1& &0\\&⋱&\\0&&1\\0&⋯&0\\⋮&&⋮\\0&⋯&0} \text{ if~$u ≥
v$,}\quad
J_{u,v} = \mat{0&⋯&0&1&&0\\⋮&&⋮&&⋱&\\0&⋯&0&0&&1} \text{ if $u ≤ v$.}
\end{equation}
\begin{enumerate}
\item For all integers~$u, v$, $H_{u} J_{u,v} = J_{u,v} H_{v}$.
\item The space of all matrices~$A$ of size~$u × v$ such that~$H_u A = A
H_v$ is exactly $k[H_u] J_{u,v}$.
\item \label{it:Juw} $J_{u,v} J_{v,w} = H_v^{d} J_{v,w}^{}$ where $d$~is
the distance between~$v$ and the interval~$[u,w]$. In particular,
$J_{u,v} J_{v,u} = H_{u}^{\abs{u-v}}$.
\end{enumerate}
\end{lem}
%>>>
\begin{prop}\label{prop:structure-commutant}%<<<
Let~$M$ be the block-diagonal matrix with diagonal blocks~$(H_{n_i} +
μ)^{♭}$. The commuting space of~$M$ is the space of all matrices~$A^{♭}$,
where $A$~is a block matrix~$A = (A_{i,j})$, with $A_{i,j} ∈
 k[H_{n_i}] J_{n_{i}, n_{j}} = J_{n_i, n_j} k[H_{n_j}]$.
\end{prop}
%>>>
According to Prop.~\ref{prop:structure-commutant}, we may replace all
elements~$A$ of~$\ro C(M)$ by their images~$A^{♯}$ in~$\ro C(M^{♯})$. We
may therefore assume that~$e = 1$, which means that~$K = k$. In this
case, as~$\ro C(M) = \ro C(M-μ)$, we may further assume that~$μ = 0$.
This means that  $M$~is the block-diagonal matrix with diagonal
blocks~$H_{n_i}$ for integers~$n_1 ≥ … ≥ n_r$.

Let~$A ∈ \ro C(M)$. Each entry~$A_{i,j}$ may be written as a polynomial
\begin{equation}
A_{i,j} = a_{i,j} (H_{n_i}) J_{n_i,n_j} = J_{n_i,n_j} a_{i,j} (H_{n_j})
\end{equation}
where $a_{i,j}(H) ∈ k[H]/H^{n_j}$.

We simplify the notation and write~$A = (a_{i,j})$ where~$a_{i,j} ∈
k[H]$. We note however that elements of~$\ro C(M)$ do not multiply as
matrices with coefficients in~$k[H]$, due to the relations
of Lemma~\ref{lem:commute-jordan}. An easy way to perform the computations is
given in Prop.~\ref{prop:phantom} below.

\begin{prop}\label{prop:phantom}%<<<
Let~$R = k[H]/H^{n_1}$. For all~$i, j$, let~$e_{i,j} = \max (0, n_i -
n_j)$. For any matrix~$A = (A_{i,j}) ∈ \ro C(M)$, where $A_{i,j} =
a_{i,j}(H_{n_i}) J_{n_i, n_j}$, define
\begin{equation}
ψ(A) = (H^{e_{i,j}}\; a_{i,j}(H)) \; ∈ \; R^{r×r}.
\end{equation}
Then $ψ$~is a $k$-algebra morphism from~$\ro C(M)$ to~$R^{r×r}$.
\end{prop}

\begin{proof}
The matrices of~$\ro C(M)$ multiply according to the relations of
Lemma~\ref{lem:commute-jordan}. We only need to prove that the
integers~$e_{i,j}$ are an integral of the exponents~$\mathrm{distance}
(n_k, [n_i, n_j])$ of Lemma~\ref{lem:commute-jordan}:
\begin{equation}\label{eq:integral}
e_{i,k} - e_{i,j} + e_{k,j} = \mathrm{distance} (n_k, [n_i, n_j]).
\end{equation}
This can be verified by checking for all orderings of the
triple~$(i,j,k)$.
\end{proof}%>>>

\paragraph{The adjunction involution.}
We now describe the adjunction involution~$A ↦ A^{⋆}$ of the commuting
space~$\ro C(M)$.

\begin{prop}\label{prop:big-T}%<<<
For each integer~$u$, write~$T_u$ for the anti-identity matrix of
size~$u$.
\begin{enumerate}
\item $T_u$~is invertible and both~$T_u$ and~$T_u H_u$ are symmetric.
\item $\transpose{J_{u,v}} T_u = T_v J_{v,u}$.
\item Let~$T$ be the block-diagonal matrix with diagonal
blocks~$T_{n_i}$. Then $T ∈ \ro S(M)^{×}$.
\end{enumerate}
\end{prop}
%>>>
\begin{prop}\label{prop:adjoint}%<<<
Let~$A = (a_{i,j}(H)) ∈ \ro C(M)$. Then \[ A^{⋆} = (a_{ji}(H)). \]
In particular, $TA$~is symmetric if, and only if, $a_{i,j} = a_{ji}$
in~$k[H]/H^{\min (n_i, n_j)}$.
\end{prop}
%>>>

As a corollary of Prop.~\ref{prop:phantom} and~\ref{prop:adjoint}, we get
the following. Let~$D$ be the diagonal matrix~$(H^{n_1-n_i}) ∈ R^{r×r}$.
For any~$A = (a_{i,j}∈ \ro C(M)$, let
\begin{equation}
φ(A) = D · ψ(A) = (H^{\max (n_1-n_i,n_1-n_j)}a_{i,j}) ∈ R^{r×r}.
\end{equation}
We then have~$φ(A^{⋆}) = \transpose{φ(A)}$. This implies that, for
all~$A, X ∈ \ro C(M)$:
\begin{equation}\label{eq:phi-psi}
φ(X^{⋆}\,A\,X) = \transpose{ψ(X)}\, φ(A)\, ψ(X).
\end{equation}

\paragraph{Structure of the commutant group.}%<<<

We show here how to compute in the commutant group~$\ro C(M)^{×}$. The
main result can be stated in two equivalent ways, as a formula to compute
the determinant of elements of~$\ro C(M)^{×}$, or as a Gaussian
elimination algorithm in the commutant group. For~$A ∈ \ro C(M)$, we
define the \emph{big block} of index~$(u,v)$ of~$A$ as the
sub-matrix~$(a_{i,j})$, where $i,j$~run over the range where~$n_i = u$
and~$n_j = v$. Prop.~\ref{prop:det-bigblock} states that $A$~is
invertible if and only if all its diagonal big blocks are invertible as
matrices with coefficients in~$k[H]/H^{n_i}$.

\begin{lem}\label{lem:det-1block}%<<<
Let~$s$ be such that~$n_1 = … = n_s > n_{s+1}$.
Define~$n'_i = n_i - 1$ if~$i ≤ s$, and~$n'_i = n_i$ if~$i > s$.

For any matrix~$A ∈ \ro C(M)$ given by the block decomposition~$A =
(a_{i,j}(H_{n_i}) J_{n_i,n_j})$, define~$A'$ as the square matrix of
size~$n - s$ given by the blocks~$(a_{i,j}(H_{n'_i}) J_{n'_i,
n'_j})$.
Then \[ \det A \;=\; \det (a_{i,j}(0))_{1 ≤ i,j ≤ s} · \det A'. \]
\end{lem}

\begin{proof}
Let~$σ$ the unique permutation of~$[1,n]$ such that~$σ(n_i) = i$ for~$i =
1, …, s$, and~$σ$ is increasing on all other indices. Then the matrix
$A^{σ}$ deduced from~$A$ by applying~$σ$ both on the lines and the
columns of~$A$ is lower block-triangular, with two diagonal blocks
respectively equal to the matrix~$(a_{i,j}(0))_{i,j ≤ s}$ and to~$A'$.
Since~$\det A = \det A^{σ}$, this proves the lemma.
\end{proof}
%>>>
\begin{prop}\label{prop:det-bigblock}%<<<
Let~$A = (a_{i,j}(H)) ∈ \ro C(M)$. Then
\[ \det A \;=\; ∏_{d ≥ 1} \det (a_{i,j}(0) | n_{i} = n_j = d)^{d} \]
where $a_{i,j}(0)$~is the image modulo~$H$ of~$a_{i,j} ∈ k[H]$.
\end{prop}

\begin{proof}
By induction on~$n_1$, with Lemma~\ref{lem:det-1block} providing the
induction step. The base case~$n_1 = 1$ corresponds to~$k[H] = k$ and
therefore~$A = (a_{i,j}(0))$.
\end{proof}
%>>>
\begin{prop}\label{prop:structure-gl}%<<<
The commutant group~$\ro C(M)^{×}$ is
generated by the following matrices:
\begin{enumerate}
\item big-block-diagonal matrices, i.e. matrices whose only non-zero big
blocks are those on the diagonal;
\item small-block transvection matrices.
\end{enumerate}
\end{prop}

\begin{proof}
By Prop.~\ref{prop:det-bigblock}, an invertible matrix~$A ∈ \ro C(M)$ has
all its diagonal big blocks invertible. Therefore we may apply the
Gaussian elimination algorithm to factor $A$~as a product~$A = LU$, where
$L$~is lower triangular with diagonal elements~$1$, and $U$~is upper
triangular.
\end{proof}%>>>
%>>>

\subsection{Classification of local symmetric pencils}%<<<2

We shall need the following classic result about bilinear forms over
local algebras.

\begin{prop}\label{prop:local-diag}%<<<
Let~$R$ be a local ring with residue field~$k$ and let~$\acco{δ_i}$ be a
set of representatives for~$k^{×}$ modulo squares. Then any regular
symmetric matrix with entries in~$R$ is congruent over~$R$ to a
diagonal matrix~$(1, …, 1, δ_i)$ for some~$i$.
\end{prop}

\begin{proof}
The Gram orthogonalization algorithm works; cf.
\cite[I(3.4)]{milnorhusemoller} or \cite[92:1]{omeara}.
\end{proof}

In the present case, since $k$~is a finite field, we have $k^{×} /
(k^{×})^2 = \acco {1, δ}$ for some element~$δ$.
%>>>
\begin{prop}\label{prop:bb-diag}%<<<
Let~$A$ be an invertible, self-adjoint element of~$\ro C(M)$. Then there
exists $X ∈ \ro C(M)^{×}$ such that $X^{*} A X$~is big-block-diagonal.
\end{prop}

\begin{proof}
We prove this by induction on the number of big blocks of~$A$, using Gram
orthogonalization on the big blocks of~$A$. Let~$B$ be the first diagonal
big block of~$A$. We may then write $A$~as a block matrix
\begin{equation}
A = \mat{B & C \\ \transpose{C} & A'}, B ∈ R^{d×d}.
\end{equation}
Since $A$~is invertible, all its diagonal big blocks are invertible; in
particular the matrices~$B$ and $A'$~are invertible. By the induction
hypothesis, there exists~$X'$ such that~$Δ = (X')^{*} A' X'$ is
big-block diagonal.

We then define
\begin{equation}
X = \mat{1 & -B^{-1} C\\0 & X'}\quad\text{and see that}\quad
X^{⋆} A X = \mat{B & 0\\0 & Δ}.
\end{equation}
\end{proof}

We note that the regularity hypothesis on~$A$ is essential for
Prop.~\ref{prop:bb-diag}. As a counter-example, assume that the Jordan
sequence is~$n_1 > n_2$ with~$n_2 ≥ 2$, and let~$A = \mat{H_{n_1} &
J_{n_1,n_2} \\ J_{n_2,n_1} & H_{n_2}} = \mat{H&1\\1&H}$.
Then, using the notations from~\eqref{eq:phi-psi}, no matrix of the
form~$ψ(X) = \mat{x_1&H x_2\\x_3&x_4}$ diagonalizes~$φ(A) =
\mat{H&H\\H&H^2}$ in~$R^{2×2}$, and therefore $A$~is not not big-block
diagonalizable by a matrix commuting with~$M$.
%>>>
\begin{prop}\label{prop:diag}%<<<
Let~$A$ be an invertible, self-adjoint element of~$\ro C(M)$. Then $A$~is
congruent to a diagonal matrix, where each diagonal big block is either
the identity matrix, or the diagonal matrix~$(1, …, 1, δ)$.
\end{prop}

\begin{proof}
Use propositions~\ref{prop:bb-diag} and~\ref{prop:local-diag}.
\end{proof}%>>>

\subsection{Solving the general case of IP1S}
\begin{thm}\label{thm:IP1S}
Let~$k$ be a finite field of characteristic~$≠2$ and~$(b_{λ})$ be a
pencil of $n$-dimensional symmetric bilinear forms over~$k$.
It is possible, using no more than~$\Ot(n^3)$ operations in~$k$, to compute
an isomorphism between~$(b_{λ})$ and a (unique) block-diagonal pencil
with diagonal blocks of the following form:
\begin{itemize}
\item Kronecker blocks~$K_{h} = \mat{0&K'_{h}\\\transpose{K'_h}&0}$
for integers~$h ≥ 0$, as defined in Prop.~\ref{prop:kronecker};
\item finite local blocks~$L_{p,d,u}$, defined as the~$d × d$-block matrix
\[ L_{p,d,u} \;=\; \mat{0&&&T_p u (λ + M_p)\\&&⋰&T_p u\\
  &⋰&⋰&\\T_pu(λ+M_p)&T_p u&&0}, \]
where $p$~is an irreducible polynomial, $M_p$~is the companion matrix
of~$p$, $T_p$~is a prescribed invertible matrix such that both~$T_p$
and~$T_p M_p$~are symmetric, $d$~is an integer, and $u$~is either the
identity matrix or a prescribed non-square element of the field~$k[M_p]$,
with the extra condition that for fixed~$(p, d)$, only one of the
values~$u$ may be different from~$1$;
\item infinite local blocks~$L_{1,d,u}$, defined as the~$d×d$-matrix
\[ L_{1,d,u} \;=\; u\mat{0&&&λ\\&&⋰&1\\&⋰&⋰&\\λ&1&&0}, \]
where $d$~is an integer, $u$~is either 1 or a prescribed non-square
element of~$k$, and for fixed~$d$, only one of the values~$u$ may be
different from~$1$.
\end{itemize}
\end{thm}

The only place where the finiteness of~$k$ is required for this theorem
to hold is for the computation of square roots, needed for
Prop.~\ref{prop:local-diag}. This theorem solves the IP1S problem in
cubic time: given two pencils~$\bm{a}$ and~$\bm{b}$, we only have to
transform both of them to the canonical form above. This form will be the
same iff the two pencils are isomorphic in the IP1S sense, and in this
case, composing the two transformations gives an answer to the
computational IP1S problem.

\paragraph{Algorithm and Complexity}

The algorithm of Theorem~\ref{thm:IP1S} for a pencil~$\bm{b}$ decomposes
in the three following steps.
\begin{enumerate}
\item \label{it:alg-kronecker} Compute the Kronecker decomposition:
as long as the kernel of the matrix~$b_{λ}$ is not trivial, compute a
minimal relation~$e = ∑ λ^i e_i$ and the according Kronecker block.
\item \label{it:alg-factor} Now~$\bm{b}$~is regular. Compute and factor
its characteristic polynomial~$f(λ)$ and split~$V$ as a orthogonal direct
sum of primary components~$V_p$ for each prime divisor~$p$.
\item \label{it:alg-local} For each prime divisor~$p$, write the local
pencil~$\bm{b}_p$ at~$p$ as a matrix with entries in~$K = k[μ]/p(μ)$.
Perform big-block reduction to write~$\bm{b}_p$ as a orthogonal direct
sum of quadratic forms over~$K[H]/H^{n_i}$, and then reduce
each of these forms to one of the two canonical diagonal forms.
\end{enumerate}

Most of the linear algebra steps, including computing the Frobenius
rational normal form of the regular part of the pencil, may be done
in~$\Ot(n^3)$ field operations~\cite{kaltoffen11compute}. In particular,
the Frobenius rational normal form covers the factoring of the
characteristic polynomial. This may also be done, again in cubic time,
using a dedicated factoring algorithm. The reduction of quadratic forms
over the local algebras is just the reduction over the residual field
(which uses a square root computation in said finite field), followed by
a Hensel lift.

The only part not covered by classic algorithms is the reduction to
Kronecker normal form performed in Step~\ref{it:alg-kronecker}. The
computation of minimal relation~$e = ∑ λ^i e_i$ amounts to a kernel
computation over the polynomial ring~$k[λ]$. This requires handling
polynomials of degree up to the minimal index~$h_1$ of~$(b_{λ})$ and
therefore has complexity~$\Ot(h_1 n^3)$. As this is performed for all
minimal indices~$h_i$, the total complexity is~$\Ot(n^4)$.

There exist cubic algorithms computing the Kronecker kernel of pencils of
linear maps over a characteristic zero field~\cite{beelen1988improved}.
These algorithms are not directly applicable over a finite field as they
use some rotations over the real numbers and are mostly concerned with
numerical stability; more importantly, they work with matrices up to
equivalence, whereas we need matrices up to congruence. However, as the
corresponding problem over a finite field has not been much studied, the
existence of a faster algorithm for computing the Kronecker kernel is not
unlikely.

%>>>1
\section{Computation of the second secret for IP2S}%<<<1
\label{S:IP2S}

\subsection{Reduction to the regular case}
Two families of polynomials~$(a_1,…,a_m)$ and~$(b_1,…,b_m)$ are
\emph{isomorphic with two secrets} if there exist bijective linear
transformations~$s$ of the $n$~variables and~$t$ of the $m$~polynomials
such that $t ∘ \bm{a} ∘ s = \bm{b}$. Assume that $m = 2$. Then the second
secret~$t$ is a homography in two variables, which we write~$γ ∈
\GL_2(k)$.

\begin{prop}
Two pencils~$\bm{a} = (a_{λ})$ and~$\bm{b} = (b_{λ})$ are isomorphic with
two secrets if, and only if, the regular part of both pencils are
isomorphic with two secrets.
\end{prop}

\begin{proof}
The minimal index of the pencil~$\bm{b}$ is the minimal degree of an
isotropic vector~$e_0 + … +λ^h e_h$ for~$b_{λ}$; such a vector may be
written in homogeneous form in~$(λ:μ)$ as~$e(λ:μ) = ∑ λ^i μ^{h-i} e_i$,
which is isotropic for the quadratic form~$b(λ:μ) = μ b_0 + λ b_{∞}$. Now
let~$γ = \smat{a&b\\c&d} ∈ \GL_2(k)$ be a homography. Then the
vector~$e^{γ}$ defined by~$e^{γ}(λ:μ) = e(aλ+bμ:cλ+dμ)$~is isotropic
for~$b_{γ(λ)}$ iff $e$~is isotropic for~$b$. This proves that the pencils
$(b_{γ(λ)}$ and~$(b_{λ})$ have the same minimal index. Therefore, all
their Kronecker blocks coincide.
\end{proof}

% \begin{prop}
% Let~$K_{λ}$ be a Kronecker block of dimension~$2h+1$ as in
% Prop.~\ref{prop:kronecker1}. Then for all~$γ ∈ \GL_2(k)$, the pencils
% with matrix~$K_{λ}$ and~$K_{γ(λ)}$ are isomorphic in the IP1S sense.
% \end{prop}
% 
% \begin{proof}
% Let~$(e_0,…,e_h; f_1,…,f_h)$ be the basis in which the pencil~$(b_{λ})$
% has the Kronecker-block matrix. Let~$e(x) = ∑ x^i e_i$ and~$f(y) = ∑
% y^{i-1} e_i$. The pencil~$(b_{λ})$ is then defined by the relation
% \begin{equation}\label{eq:def-b-intr}
% b_{λ} (e(x), f(y)) \;=\; (x+λ) F(xy), \quad F(t) = 1 + … + t^{h-1}.
% \end{equation}
% For any homography~$γ = \smat{a&b\\c&d} ∈ \GL_2(k)$, define a
% basis~$(e_i^{γ}, f_i^{γ})$ by the relations
% \begin{equation}\label{eq:def-ei-gamma}
% ∑ (a λ + b μ)^i (c λ + d μ)^{h-i} e_i = ∑ λ^i μ^{h-i} e_i^{γ}.
% \end{equation}
% We then find that in the basis 
% 
% ; then the only relation is~$b_{λ} · ∑ λ^i
% e_i = 0$, which we write as a homogeneous polynomial
% in~$(λ:μ)$ as $(λ b_{∞} + μ b_{0}) (∑ λ^i μ^{h-i} e_i) = 0$.
% Let~$γ = \smat{a&b\\c&d}$ and define a basis~$(e_i^{γ}, f_i^{γ})$ by
% Then in the basis
% \end{proof}


\subsection{IP2S in the regular case}

Let~$(a_{λ})$ and~$(b_{λ})$ be two \emph{regular} pencils of bilinear
forms such that $a_{γ(λ)}$~is isomorphic, in the IP1S sense, to~$b_{λ}$.
Then the homography~$γ$ maps the characteristic polynomial~$f(λ)
= \det (a_{λ})$ to~$g(λ) = \det (b_{λ})$. In particular, it maps the
prime factors of~$f$ to those of~$g$, respecting both their degree and
their exponent as a factor of the characteristic polynomial.

Let~$S_{d,e}$~and~$T_{d,e}$ be the set of factors of degree~$d$ and
exponent~$e$ of the polynomials~$f$ and~$g$. Then any homography~$γ$
mapping all the elements of~$S_{d,e}$ to~$T_{d,e}$ for each pair~$(d,e)$
is a possible second secret in the IP2S problem. We compute the
intersection for~$(d,e)$ of the set~$Γ_{d,e}$ of homographies mapping the
prime polynomials of~$S_{d,e}$ to~$T_{d,e}$. In most cases, the first
set~$Γ_{d,e}$ already contains only one candidate, which is therefore the
second secret~$γ$. The discussion depends on the degree~$d$ of the
polynomials. We note that the sum of the size of the sets~$S_{d,e}$ is
the number of variables~$n$; therefore, we may use the worst-case
estimate~$\card{S_{d,e}} = O(n)$ for each~$(d,e)$.

We shall use the following classic results.
\begin{prop}\label{prop:homography}
\begin{enumerate}
\item Let~$(x_1, x_2, x_3)$ and~$(y_1, y_2, y_3)$ be two (ordered)
triples of distinct points of~$ℙ^1(k)$. There exists a unique
homography~$γ ∈ \mathrm{PGL}_2(k)$ such that~$γ(x_i) = y_i$.
\item Let~$(x_1, x_2, x_3, x_4)$ and~$(y_1, y_2, y_3, y_4)$ be two
(ordered) quadruplets of distinct points. They are homographic iff they
have the same cross-ratio~$B(x) = B(y)$, where
\begin{equation}
B(x) = \frac{(x_1-x_3)(x_2-x_4)}{(x_1-x_4)(x_2-x_3)}.
\end{equation}
\item Let~$\acco{x_1, x_2, x_3, x_4}$ and~$\acco{y_1, y_2, y_3, y_4}$ be
two (unordered) sets of four points. They are homographic iff they have
the same $j$-invariant~$j(x) = j(y)$, where
\begin{equation}\label{eq:j-invariant}
j(x) = \frac{(B(x)^2-B(x)+1)^3}{B(x)^2(1-B(x))^2}.
\end{equation}
\item Let~$u(x) = ∑ u_i x^i$ and~$v(x)$ be two monic polynomials
of degree four. They are homographic iff they have the same $j$-invariant,
where $j(u)$~is a polynomial of degree~$6$ in the coefficients of~$u$.
\end{enumerate}
\end{prop}

We note that the formula for the $j$-invariant given
in~\eqref{eq:j-invariant} is, up to a constant factor, the formula for
the $j$-invariant of an elliptic curve. Namely, two elliptic curves with
equations~$y^2 = f(x)$ and~$y^2 = g(x)$, where $f, g$ are separable
polynomials of degree~$≤ 4$, are isomorphic iff the polynomials~$f$
and~$g$ are homographic.

\bigbreak
We now explain how we compute the set~$Γ_{d,e}$ for each pair~$(d,e)$.

\paragraph{Case~$d = 1$.}
If $\card{S_{1,e}} ≥ 3$, then we may immediately recover the
homography~$γ$: namely, fix a triple~$(x_1,x_2,x_3)$ in~$S_{1,e}$, and
iterate over the triples in~$T_{1,e}$. For each such triple, there exists
a unique homography~$γ$ such that~$γ(x_i) = y_i$. This homography belongs
to~$H_{1,e}$ iff the images of all the other points of~$S_{1,e}$ belong
to~$T_{1,e}$. Since there are~$3!\binom{\card{S_{1,e}}}{3} = O(n^3)$
triples~$(y_i)$, this computation requires~$O(n^3)$ field operations.

If $1 ≤ \card{S_{1,e}} ≤ 2$, then $H_{1,e}$~may be explicitly computed as
the union of the set of homographies mapping the elements of~$S_{1,e}$ to
those of~$T_{1,e}$ for all permutations of~$T_{1,e}$.

\paragraph{Case~$d = 2$.}
Assume $\card{S_{2,e}} ≥ 2$. Let~$u_1, u_2 ∈ S_{2,e}$ and~$v_1, v_2 ∈
T_{2,e}$ be monic polynomials of degree~two. Any homography between the
sets~$\acco{u_1, u_2}$ and~$\acco{v_1, v_2}$ will map~$u_1 u_2$ to~$v_1
v_2$. By Prop.~\ref{prop:homography}(iv), there exists at most a bounded
number of such homographies. Since there are~$\binom{\card{S_{2,e}}}{2} =
O(n^2)$ pairs~$(v_1, v_2)$, this requires~$O(n^2)$ field operations.

If~$\card{S_{2,e}} = 1$, then $H_{2,e}$~is the set of all homographies
mapping the unique element of~$S_{2,e}$ to the unique element
of~$T_{2,e}$.

\paragraph{Case~$d = 3$.}
Fix an element~$u ∈ S_{3,e}$. For all~$v ∈ T_{3,e}$, there exist at
most~$3! = 6$ homographies~$γ$ mapping~$u$ to~$v$. Each candidate belongs
to~$H_{3,e}$ iff it maps all other elements of~$S_{3,e}$ to elements
of~$T_{3,e}$. There are~$\card{S_{3,e}} = O(n)$ candidates~$u$ and
therefore~$O(n)$ candidate homographies~$γ$.

\paragraph{Case~$d = 4$.}
Fix an element~$u ∈ S_{4,e}$. The candidates as homographic images of~$u$
in~$T_{4,e}$ are the~$v$ such that~$j(v) = j(u)$. Each candidate
polynomial~$v$ gives at most $4! = 24$~candidates homographies~$γ$. This
allows to compute~$H_{4,e}$ in~$O(n)$ field operations.

\paragraph{Case~$d ≥ 5$.} The naïve method is to differentiate~$(d-4)$
times the elements of~$S_{d,e}$ to reduce to the case where~$d = 4$.
However, as this uses only the five leading coefficients, if the
polynomials are specially chosen we may find too many homographies; for
example, although the polynomials~$x^d-1$ and~$x^d$ are not homographic,
all their derivatives are. Instead, we first compose all the elements
of~$S_{d,e}$ and~$T_{d,e}$ by a known, randomly chosen homography~$r$. In
general, for any two non-homographic elements~$u_1, u_2 ∈ S_{d,e}$, the
derivatives~$(∂/∂x)^4\, (u_i ∘ r)$ are non-homographic. In the improbable
case where they are homographic, we only need to change the random
homography~$r$. In this way, we may compute the set~$Γ_{d,e}$ in at
most~$O(n)$ field operations.

\paragraph{Computing the hidden homography.}

The hidden homography~$γ$ lies in the intersection of all sets~$Γ_{d,e}$.
As each one of these sets is likely to be extremely small or even reduced
to~$\acco{γ}$, we compute them in increasing order of assumed complexity.
We use the above estimates: for each~$(d,e)$, we use the assumed complexity
\begin{equation}
C_{d,e} = \begin{cases}
\card{S_{d,e}}^3,& d = 1;\\
\card{S_{d,e}}^2,& d = 2;\\
\card{S_{d,e}},& d≥ 3,
\end{cases}
\end{equation}
and sort the pairs~$(d,e)$ by increasing values of~$C_{d,e}$. We finally
find a bounded number of candidate homographies using no more
than~$O(n^3)$ operations in~$k$.

% Conclusion
\section*{Conclusion}

In this paper, we show that we can solve in polynomial-time the IP
problem with two quadratic forms in a finite field of odd characteristic.
The obvious questions are whether it is possible to generalize this to
fields of characteristic two and to more than two equations.

The case of a binary base field is very important for cryptographical
applications. The cyclic case has been solved in~\cite{MPG2013}. To solve
the general case, at least three roadblocks remain: the Kronecker form
for the singular part of a bilinear pencil does not apply, as the proof
of Prop.~\ref{prop:kronecker1} above requires that~$2 ≠ 0$; quadratic
forms over a local algebra behave differently~\cite[§93]{omeara};
finally, extending from bilinear to quadratic forms requires a study of
the action of a symplectic group on the diagonal coefficients, and this
group becomes quite impractical in the non-cyclic case.

On the other hand, studying the general problem with $m ≥ 3$~quadratic
equations departs from the classic results about pencils of quadratic
forms; therefore, there are less tools available. Even in the regular
case, our work heavily uses the factorization of the characteristic
polynomial. The analogous case for~$m ≥ 3$ requires working in
homogeneous polynomials in~$m$ variables, which is a less pleasant
algebra.

% Biblio<<<1
\bibliographystyle{plain}
\bibliography{biblio}

\iffalse
\appendix
\section{Canonical pencil in normal form}
We first would like to introduce and define a canonical representative $L_\lambda$ of the class of all symmetric pencils that are equivalent to $A_\lambda$, which depends only on the set of elementary divisors of $A_{\lambda}$. We require that the symmetric pencil $L_\lambda$ be block-diagonal and that each diagonal block has only one elementary divisor. 
Let $p^d$ be one of the elementary divisors of $A_{\lambda}$. We may require that the corresponding expected block in $L_\lambda$ has the following expression: $T(\lambda +M)$ with $M$ and $T$ expressed as $d^2$-block matrices:
\begin{equation}
\label{eq:canonical}
M=\begin{pmatrix}
M_p & I & & 0\\
 & \ddots & \ddots & \\
 & & \ddots & I \\
0 & & & M_p\\
\end{pmatrix}
,\quad 
T =\begin{pmatrix}
0& & & T_p\\
& & \iddots & \\
& \iddots & & \\
T_p & & & 0\\	
\end{pmatrix},
\end{equation}
where $M_p$ is the companion matrix of $p$ and $T_p$ as defined in \eqref{eq:companion}.
%, $N$ being the companion matrix of $p$. 
One can easily check that $M$ and $T$ have the following properties: $M$ has $p^d$ as its only elementary divisor, $T$ is non singular symmetric and satisfies $\transpose{M}T=TM$.

From now, $L_\lambda=T(\lambda + M)$ refers to the whole block-diagonal pencil and is so by construction symmetric, equivalent to $A_\lambda$ representing the pencil, and uniquely defined except for the order of the elementary divisors.
\begin{prop}
\label{transpose-symmetric}
For any matrix $M$, 
%matrices $M$ and $\transpose{M}$ are similar and 
there exists a non singular symmetric matrix $T$ such that $\transpose{M}T = TM$.
\end{prop}

\begin{proof}%[of Th.~\ref{transpose-symmetric}]
Given $p=\lambda^n+\sum_{i=0}^{n-1}{p_i\lambda^i}$ a polynomial of degree $n$ over $k$, we define the two 
$n\times n$ matrices $M_p$ (the companion matrix of the polynomial $p$) and $T_p$ as:
\begin{equation}
\label{eq:companion}
M_p = \begin{pmatrix}
0 & 1 & & 0\\
\vdots & \ddots & \ddots & \\
0 & \cdots & 0 & 1 \\
-p_0 & \cdots & -p_{n-2} & -p_{n-1}\\
%
%	0 & \cdots & 0 & -p_0\\
%	1 &\ddots &\vdots & \vdots \\
%	  & \ddots & 0 & -p_{n-2}\\
%	  & & 1 & -p_{n-1}
\end{pmatrix}
, \quad T_p = \begin{pmatrix}
	p_1 & \cdots & p_{n-1} & 1 \\
	\vdots	& \iddots & \iddots & \\
	p_{n-1} & \iddots & & \\
	1 & & & 0
\end{pmatrix}
\end{equation}
One can easily verify that for any polynomial $p$, $M=M_p$ and $T=T_p$
satisfy the relation $\transpose{M}T = TM$ and $T$ is obviously non singular and symmetric. Therefore the theorem is proven for any companion matrix. The result can easily be generalized when $A_\lambda$ is in a normal form: since $M$ is block-diagonal and each block is the companion matrix of some elementary divisor, $T$ can also be built up as block-diagonal, each block being non singular symmetric. Then the result can be generalized to all matrices: for any matrix $N$ there exist a matrix $M$ in a normal form and a non singular matrix $P$ such as $N=P^{-1}MP$.  Since we have from previous step: $\transpose{M} = TMT^{-1}$ for some symmetric non-singular matrix $T$, we easily get $\transpose{N}=(\transpose{P}TP)N(\transpose{P}TP)^{-1}$ and $\transpose{P}TP$ is obviously non singular symmetric.
%\qed
\end{proof}

We easily get the following results.

\begin{prop}
\label{cor:symmetric-similar-pencil}
Let $M$ and $T$ be matrices as in Prop~\ref{transpose-symmetric}. Then $T(\lambda I+M)$ is obviously a symmetric pencil with the same invariant polynomials as $M$.
\end{prop}
\fi

\end{document}%<<<1

% \section{In characteristic two}%<<<1
% \subsection{Action of the commutant on diagonal matrices}
% 
% Let~$u$ be an integer. We define a $k-$linear map~$λ_u$ from $u ×
% u$-matrices to~$R_u = k[H]/H^u$ in the following way:
% \begin{equation}
% λ_u((a_{i,j})_{i,j=1,…,u}) = ∑_{i=1}^{u} a_{i,i} H^{i-1}.
% \end{equation}
% 
% \begin{prop}
% x_i^2 H^i$. Then for all~$x ∈ k[H_u]$,
% \begin{equation}
% λ_u (\transpose{x} \, A\, x) = φ(x)\, λ_u(A).
% \end{equation}
% \end{prop}
% 
% 
% \begin{proof}
% Given that $λ_u$~is $k$-linear, it is enough to prove that
% $λ_u(\transpose{H_u} \, A\, H_u) = H λ_u(A)$.
% \end{proof}
% 
% 
% \begin{lem}
% Let~$u, v$ be two integers and~$A ∈ R^{u × u}$.
% \begin{equation}
% λ_v ( \transpose{J_{u,v}}\, A\, J_{u,v}) \;=\;
% H^{\max (v-u, 0)}\,λ_u(A).
% \end{equation}
% \end{lem}
% 
% 
% \begin{prop}
% Let~$n_1 ≥ … ≥ n_r$. For any symmetric matrix~$A$ written as
% blocks~$A_{i,j}$ of size~$n_i × n_j$, define~$Λ(A)$ as the line matrix of
% length~$r$ with coefficients in~$R_1 = k[H]/H^{n_1}$:
% \begin{equation*}
% Λ(A) = ( H^{n_1 - n_i}\, λ_{n_i} (A_{i,i}))_{i=1,…, r}.
% \end{equation*}
% Then, for any~$X ∈ \ro C(M)$:
% \begin{equation}
% Λ(\transpose{X}\,A\,X) = Λ(A)\, φ(Ψ(X)).
% \end{equation}
% \end{prop}
% 
% 
% \begin{prop}
% Let~$A = \mat{Δ_1 & TM \\ 0 & Δ_2}$ and~$X = \mat{X_{1,1} & X_{1,2}\\
% X_{2,1} & X_{2,2}}$, where~$X_i ∈ \ro C(M)$ and~$Δ_i$ are diagonal.
% 
% Then $\transpose{X} A X$ has diagonal
% \[ Λ(\transpose{X} A X) = ( Λ(Δ_1) φΨ(X{1,i}) + Λ(Δ_2) φψ(X_{2,i}) +
%   Λ(\transpose{X_{1,i}} TM X_{2,i}))_{i=1,2}. \]
% \end{prop}
% 


\end{document}
% vim: fdm=marker fmr=<<<,>>>:

