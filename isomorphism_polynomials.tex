\documentclass{lms}%<<<1
%\usepackage[margin=30mm]{geometry}
%\usepackage[letterpaper,hmargin=1.4in,vmargin=1.4in]{geometry}
\usepackage{unicode}
\usepackage{amsmath,amssymb,mathrsfs}
\usepackage{bm}


% Theorems, equations, counters%<<<
\def\linkcounter#1#2{\edef\magic{\noexpand\let
  \expandafter\noexpand\csname c@#1\endcsname
  \expandafter\noexpand\csname c@#2\endcsname}\magic}
\def\newthm#1#2{\newtheorem{#1}{#2}[section]\linkcounter{#1}{equation}}
\newthm{prop}{Proposition}
\newthm{thm}{Theorem}
\newthm{lem}{Lemma}

\def\labelenumi{(\roman{enumi})} \let\itemref\ref
%>>>
% <<< Put \qed where it belongs
\makeatletter
\def\qed{~\ensuremath{\square}}
\let\old@proof\proof \let\old@eproof\endproof
\newif\ifqed
\def\proof{\qedtrue\old@proof}
\def\endproof{\ifqed\leavevmode\unskip~\qed\fi}
\newif\ifqed@math
\def\endmathdisplay#1{%
  \ifmmode \else \@badmath \fi
  \endmathdisplay@a
  $$%
  \ifqed@math \vadjust{\vbox to 0pt{\vskip -1.7\baselineskip
  \llap{\qed\hskip 2.2em \hskip -\hsize}\vss}}\global\qed@mathfalse \fi
  \global\let\df@label\@empty \global\let\df@tag\@empty
  \global\tag@false \global\let\alt@tag\@empty
  \global\@eqnswfalse
}
\def\qedhere{\ifmmode\global\qed@mathtrue\else\qed\fi\global\qedfalse}
\makeatother
%>>>
% Misc. math stuff%<<<
\def\bigperp{\mathop{\vcenter{\hbox{\scalebox{2}{\ensuremath{\perp}}}}}%
  \displaylimits}
\let\fr\mathfrak
\let\ro\mathscr
\def\transpose{\,{}^{\mathrm{t}\!}}
\def\acco#1{\left\{#1\right\}}
\def\abs#1{\left|#1\right|}
\def\mat#1{\begin{pmatrix}#1\end{pmatrix}}
\def\card#1{\abs{#1}}
%>>>
%>>>1

\begin{document}
\title{Isomorphism of Polynomials Problem with two equations}%<<<1
%\author{Pierre-Alain Fouque \and Gilles Macario-Rat \and Jérôme Plût}
\author{Submission to ANTS 2014}
\maketitle

\begin{abstract}
In this paper, we study the Isomorphism of Polynomial (IP) problem on
quadratic polynomials with $n$ variables over a finite field of odd
characteristic and two equations, \textit{i.e.} given two quadratic
polynomials $(\bm{a},\bm{b})$ on $n$ variables, find two bijective linear
maps $(s,t)$ such that $\bm{a}=t\circ \bm{b}\circ s$. We show a
polynomial-time algorithm that allows to recover $s$ and $t$ in $O(n^3)$.
This problem has been introduced in cryptography by Patarin back in 1996.
One special instance of this problem when $t$ is not present is called
the IP1S problem. At ASIACRYPT 2013, Macario-Rat, Pl\^ut and Gilbert
explained well behavior of generic solvers, such as Gr\"obner basis, to
solve the IP1S problem in general by describing a polynomial-time
algorithm for solving "cyclic instances" of this problem using pencil of
quadratic forms in all finite fields. They show that this problem is
reducible to computing a square root of a matrix in the commuting space,
which is a commutative algebra for "cyclic" instances. Here, we solve all
instances of the problem by describing a new reduction algorithm for
pencils of bilinear forms in a non-commutative algebra. Finally, we show
that the general IP problem, a.k.a. IP2S problem is solvable in cubic
time.
\end{abstract}

\section{Introduction}
The Isomorphism of polynomial problem has been introduced by Patarin in~\cite{DBLP:conf/eurocrypt/Patarin96} to construct an efficient authentication scheme as an alternative to the Graph Isomorphism Problem used by . 

parler du problem IP Patarin et lien avec la crypto
Cas difficile $m=2$

\paragraph{Previous Work.}
1. parler du travail de Gilles + Henri et Jerome a Asiacrypt cas cyclic

2. parler du travail de Jeremy + Faugere et Ludovic

\paragraph{Our contributions.}

\section{The Isomorphism of Polynomial Problem with One Secret}
Voir avec Jerome ce qu'on met ici en fonction du niveau des personnes

\subsection{Notations and definitions}

\subsection{The quadratic IP1S problem}

\subsection{Mathematical Background}
%>>>1
\section{IP1S in the possibly singular case}%<<<1

Throughout this document, $k$~is a field such that~$2 ∈ k^{×}$. Let~$V$
be a vector space. An \emph{(affine) pencil of symmetric bilinear forms}
over~$V$, or a \emph{symmetric pencil} in shorthand, is an affine map
from~$k$ to the space of symmetric bilinear forms over~$V$. We may write
it as~$b_{λ} = b_{0} - λ b_{∞}$, where~$λ ∈ k$ and~$b_{0}, b_{∞}$ are
symmetric bilinear forms.

The \emph{characteristic polynomial} of the symmetric pencil~$(b_{λ})$ is
the determinant~$\det b_{λ}$, as a polynomial in~$λ$. The
pencil~$(b_{λ})$ is called \emph{regular} if its characteristic
polynomial is not identically zero, and \emph{singular} otherwise. We
solve the isomorphism problem for regular pencils in section 2 below. We
reduce to this case by proving that the singular part of a symmetric
pencil is reducible to canonical form of Kronecker, described for pencils
over the complex numbers in~\cite[XII(56)]{Gantmacher2}. We give here a
proof that applies to any field where~$2 ≠ 0$ and which is algorithmic.

\subsection{Reduction of (possibly singular) pencils to the Kronecker form} %<<<2
% Définition: indice minimal%<<<
The pencil~$(b_{λ})$ defines a symmetric bilinear form on the
module~$V_{∞} = V ⊗_{k} k[λ]$; if $(b_{λ})$~is singular, then this form
has a non-trivial kernel~$W$. Elements of~$W$ are called \emph{relations}
of~$(b_{λ})$. An element~$x = x_0 + λ x_1 + … + λ^d x_d$ is a
relation iff
\begin{equation}\label{eq:relation}
b_0 x_0 = 0, \quad
b_0 x_1 = b_{∞} x_0, \quad …
b_0 x_d = b_{∞} x_{d-1}, \quad
b_{∞} x_{d} = 0.
\end{equation}
A \emph{minimal relation} for~$(b_{λ})$ is one with minimal degree~$d$;
this degree is the \emph{minimal index} of~$(b_{λ})$. A basis
of~$W$ adapted to the filtration of~$V_{∞}$ by the degree of polynomials,
we see that $W$~as a basis~$(w_1,…,w_r)$ such that, if $d_i$~is the
degree of the relation~$w_i$, then $(w_i,…,w_r)$~generate no relation of
degree~$< d_i$. The degree~$d_i$ are called the \emph{minimal indices} of
the pencil~$(b_{λ})$.
%>>>
\begin{prop}\label{prop:minimal-indep}%<<<
Let~$e = ∑ λ^i e_i$ be a minimal relation for~$(b_{λ})$. Then the
vectors~$e_0,…,e_{d}$ are $k$-linearly independent.
\end{prop}

\begin{proof}
Assume that~$∑ α_i e_i = 0$ is a non-zero relation between the~$e_i$.
Then the family~$(e'_i)$ defined by~$e'_i = α_{m-i} e_0 + … + α_m
e_{i}$ for~$i = 0, …, m-1$ satisfies the relations
\begin{equation}\label{eq:relation-e'}
\begin{split}
b_0 e'_{i} = α_{m-i+1} b_0 e_1 + … + α_{m} b_0 e_{i+1} = b_{∞} e'_{i-1},
\\
b_0 e'_0 = α_0 b_0 e_0 = 0,
\quad b_{∞} e'_{m-1} = b_0(α_0 e_0 + … + α_m e_m) = 0.
\end{split}\end{equation}
In other words, $(e'_0,…, e'_{m-1})$~is a relation of degree~$(m-1)$
of~$(b_{λ})$, which contradicts the minimality of~$e$.
\end{proof}%>>>
\begin{prop}\label{prop:minimal-matrix}%<<<
Let~$n = n'+d+1$ where $d$~is the minimal index, and let~$e = ∑ λ^i
e_i$ be a minimal relation for~$(b_{λ})$. In any basis of~$V$
completing~$(e_i)$, the pencil~$(b_{λ})$ has the matrix
\begin{equation}\label{eq:minimal-matrix}
B_{0} = \mat{0 & \transpose{C_0}\\ C_0 & B'_{0}}, \quad
B_{∞} = \mat{0 & \transpose{S} \transpose{C_0}\\ CS & B'_{∞}}.
\end{equation}
where $0$~is a square block of size~$(d+1)$, $C_0$~is a matrix of size~$n'
× (d+1)$ with null first column and rank~$d$, $S$~is the right-shift
matrix of size~$d+1$, and $B'_{∞}, B'_0$~are symmetric matrices of
size~$n'$.
\end{prop}

\begin{proof}
Let~$B_{0} = (a_{i,j})$ and~$B_{∞} = (b_{i,j})$ for~$0 ≤ i,j ≤ n-1$. The
relation~$B_{0} e_{i+1} = B_{∞} e_{i}$ imply that $a_{i,j+1} = b_{i,j}$.
Since both matrices are symmetric, we may write
\begin{equation}\label{eq:zero-bij}
b_{i,j} = a_{i,j+1} = a_{j+1, i} = b_{j+1, i-1} = b_{i-1, j+1}.
\end{equation}
From this and the fact that~$a_{i,0} = a_{0,i} = 0$ and~$b_{i, d} =
b_{d,i} = 0$, we deduce that~$a_{i,j} = b_{i,j} = 0$ for all~$0 ≤ i, j ≤
d$.

This means that both matrices~$B_{0}$ and~$B_{∞}$ may be written as
in~\ref{eq:minimal-matrix}, where it only remains to prove that $C_0$~has
rank~$d$.

\medskip

Since $d$~is minimal, this means that there exist no non-trivial
relation~$(B_0 - λ B_{∞}) (x_0 + … + λ^{d-1} x_{d-1}) = 0$. In other
words, the block-matrix
\begin{equation}
\def\arraystretch{.8}
\ro B = \mat{B_0\\-B_{∞} & B_0\\ & -B_{∞} & \smash{⋱}\\
& & \smash{⋱} \\ & & & B_0\\ & & & -B_{∞}}
\end{equation}
is injective. Since $\ro B$~has as a sub-matrix the matrix
\begin{equation}
\def\arraystretch{.8}
\ro C = \mat{C_0\\-C_0 S & C_0\\ & -C_0 S & \smash{⋱}\\
& & \smash{⋱} \\ & & & C_0\\ & & & -C_0 S},
\end{equation}
the matrix~$\ro C$ is also injective. This matrix has~$(d+1) × d$
blocks of size~$n' × (d+1)$, and its rank is thus~$d(d+1)$. The
matrix~$\ro C$ has as a left factor the
block-diagonal matrix with its~$(d+1)$ diagonal blocks equal to~$C_0$;
therefore, the rank of~$C_0$ must be at least~$d$.
\end{proof}%>>>

From Prop.~\ref{prop:minimal-matrix}, using a change of basis, we deduce
that there exists a basis in which the pencil~$(b_{λ})$ has the matrix
\begin{equation}
B_0 = \mat{0 & \transpose{C_0} & 0 \\ C_0 & 0 & 0 \\ 0 & 0 & B'_0},\qquad
B_{∞} = \mat{0 & \transpose{S} \transpose{C_0} & 0 \\
  C_0 S_0 & 0 & 0 \\ 0 & 0 & B'_0}.
\end{equation}

\begin{thm}[(Kronecker form)]
Let~$(b_{λ})$ be a symmetric pencil on~$V$. There exists a basis of~$V$
in which the pencil has a block-diagonal matrix with diagonal
blocks~$(L_{d_1}, …, L_{d_r}, B')$, where $L_d$~is the square matrix of
size~$2d+1$ defined by
\begin{equation}
\def\arraystretch{.7} L_{d} = \mat{
 & & & &λ& &0\\
 &0& & &1&⋱& \\
 & & & & &⋱&λ\\
 & & & &0& &1\\
λ&1& &0& & & \\
 &⋱&⋱& & &0& \\
0& &λ&1& & & \\
},
\end{equation}
the integers~$d_1 ≤ … ≤ d_r$ are the minimal indices of~$(b_{λ})$, and
$B'$~is the matrix of a regular pencil.
\end{thm}
Note that in particular, the matrix~$L_{0}$ is the zero matrix of
size~$1×1$. The appearances of~$L_0$ in the Kronecker form correspond to
the constant vectors in the kernel of~$B_{λ}$.

\section{IP1S for regular pencils}%<<<1
\subsection{Localization of regular pencils}%<<<2


\begin{prop}\label{prop:decomp-bezout}
Let~$S$ be a regular symmetric pencil on the vector space~$V$.
Let~$f(λ)$~be the characteristic polynomial of~$S$, and let~$f = ∏
g_i$ be a factorization of~$f$ in mutually coprime factors.

Then there exists a unique decomposition~$V = ⨁ V_i$ such that the
spaces~$V_i$ are pairwise orthogonal for each form of~$S$ and the
restriction~$S|_{V_i}$ has characteristic polynomial~$g_i$.
\end{prop}

\begin{proof}
It is enough to prove this when $f$~is the product of two factors~$f =
gh$. Let~$x,y ∈ V$ such that~$
\end{proof}

\subsection{From symmetric to commuting matrices}%<<<2
\subsection{The adjunction involution}%<<<2
\subsection{Classification of local symmetric pencils}%<<<2
%>>>1
% Biblio<<<1
\bibliographystyle{plain}
\bibliography{biblio}
\end{document}%<<<1
\section{Local structure of symmetric pencils}%<<<1
\subsection{From symmetric to commuting matrices}%<<<2

Let~$k$ be a field. A \emph{symmetric pencil} is an affine line in the
space of symmetric bilinear forms with coefficients in~$k$:
\begin{equation}
b_{λ} = λ S_{∞} + S_0.
\end{equation}
Up to a change of the basis~$(S_{∞}, S_{0})$ of~$S$, we may assume that
$S_{∞}$ divides~$S_0$ on the left. Defining~$M = S_{∞}^{-1} S_0^{}$, we
may therefore write the pencil~$(S_{λ})$ as
\begin{equation}
S_{λ} = S_{∞} (λ + M),
\end{equation}
where $S_{∞}$ and~$S_{∞} M$ are symmetric matrices. We say that $(S_{λ})$
is \emph{regular} if $S_{∞}$~is invertible. The matrix $M$~is the
\emph{characteristic endomorphism} of the pencil~$(S_{λ})$.

The image of the pencil~$(S_{λ})$ by a linear change of basis~$X$
is~$(S'_{λ}) = (\transpose{X} S_{λ} X)$; its characteristic endomorphism
is~$M' = X^{-1} M X$. In particular, $M = M'$ exactly when~$X$ and~$M$
commute.


For any matrix~$M$, we define the \emph{symmetrizing space}~$\ro S(M)$
and the \emph{commutant}~$\ro C(M)$ as
\begin{gather}
\ro S(M) = \acco{\text{$S$ symmetric such that $SM$~is symmetric} },\\
\ro C(M) = \text{\text{$X$ such that~$XM = MX$}}.
\end{gather}
The invertible elements of~$\ro C(M)$ form the \emph{commutant
group}~$\ro C(M)^{×}$.

In the particular case where $M$~is cyclic, the commutant~$\ro C(M)$ is
the (commutative) polynomial algebra~$k[M]$. In this case, the IP1S
problem may be solved in a straightforward way~\cite{MPG2013}. We are
interested here in the general case, where $\ro C(M)$~is usually not a
commutative algebra.

\begin{prop}\label{prop:structure-sym}
Let~$M$ be any matrix.
\begin{enumerate}
\item \label{it:sym-inv} The set~$\ro S(M)$ contains an invertible
matrix~$T$.
\item \label{it:sym-comm} For any invertible~$T ∈ \ro S(M)$ and any~$A$,
the matrix $TA$~belongs to~$\ro S(M)$ if and only if $TA$~is symmetric
and $A ∈ \ro C(M)$.
\item Any symmetric pencil with characteristic endomorphism~$M$ is of the
form~$S_{λ} = TA (λ + M)$ where $A ∈ \ro C(M)$. Moreover, it is regular
if and only if $A$~is invertible.
\item Let~$S_{λ} = TA (λ + M)$ be a symmetric pencil and~$X ∈ \ro C(M)$.
Then
\begin{equation*}
\transpose{X} S_{λ} X = T \:(X^{⋆} A X)\: (λ + M), \quad
\text{where $X^{⋆} = T^{-1} \transpose{X} T ∈ \ro C(M)$.}
\end{equation*}
\item Let~$A ∈ \ro C(M)$. Then $\transpose{(TA)} = TA^{⋆}$. In
particular, $TA$~is symmetric if, and only if, $A = A^{⋆}$.
\end{enumerate}
\end{prop}

\begin{proof}
Point~\itemref{it:sym-inv} is explicitly proven in Prop.~\ref{prop:big-T}
below. Assuming that $TA$~is symmetric, point~\itemref{it:sym-comm}
follows from
\begin{equation}
\transpose{(TAM)} \;=\; \transpose{M}\, \transpose{(TA)}
 \;=\; \transpose{M}\,TA = TMA;
\end{equation}
since $T$~is invertible, it is cancellable in the resulting equation~$TMA
= TAM$.
\end{proof}

We call~$A^{⋆}$ the \emph{adjoint} of~$A$ and say that $A$~is
\emph{self-adjoint} if~$A^{⋆} = A$. (Since we don't plan to equip the
field~$k$ with an involution, no confusion with the usual meaning is
possible).

\begin{thm}\label{thm:IP1S-congruence}
The congruence problem for regular symmetric pencils is equivalent to the
following: given two invertible self-adjoint matrices~$A, A' ∈ \ro C(M)$,
compute a matrix~$X ∈ \ro C(M)$ such that~$X^{⋆} A X = A'$.
\end{thm}

\subsection{The commuting space of a Jordan matrix}%<<<2

By using the primary decomposition of~$M$ (= Chinese remainders) as well
as an extension of scalars, we may assume that the minimal polynomial
of~$M$ is a power of a prime polynomial. Using an extension of scalars, we
assume that it is a power of~$x$.

In this case, $M$~decomposes as Jordan blocks as follows: for each
integer~$u$, let~$H_u$ be the companion matrix of the polynomial~$x^u$.
Then there exists a decreasing sequence of integers~$n_1 ≥ … ≥ n_r$ such
that~$∑ n_i = n$ and $M$~is block-diagonal, with the $i$-th block being
equal to~$H_{n_i}$.

For each integers~$u ≥ v$, define matrices~$J_{u,v}$ and~$J_{v,u}$, of
respective sizes~$u × v$ and~$v × u$, by the block decompositions:
\begin{equation}
J_{u,v} = \mat{ \mathrm{id}(v) \\ 0^{v ×(u-v)}},\quad
J_{v,u} = \mat{ 0^{(u-v)× v } & \mathrm{id}(v)}.
\end{equation}

\begin{prop}\label{prop:rel-HJ}
Let~$u, v, w$ be three integers.
\begin{enumerate}
\item $H_{u} J_{u,v} = J_{u,v} H_{v}$.
\item The space of all matrices~$A$ of size~$u × v$ such that~$H_u A = A
H_v$ is exactly $k[H_u] J_{u,v}$.
\item \label{it:Juw} $J_{u,v} J_{v,w} = H_v^{d} J_{v,w}^{}$ where $d$~is
the distance between~$v$ and the interval~$[u,w]$. In particular,
$J_{u,v} J_{v,u} = H_{u}^{\abs{u-v}}$.
\end{enumerate}
\end{prop}

\begin{prop}\label{prop:structure-commutant}
Let~$M$ be the block-diagonal matrix with diagonal blocks~$H_{n_i}$.
The commuting space of~$M$ is the space of
all block matrices~$A = (A_{i,j})$, where $A_{i,j}$~belongs
to~$k[H_{n_i}] J_{n_{i}, n_{j}} = J_{n_i, n_j} k[H_{n_j}]$.
\end{prop}

Each entry~$A_{i,j}$ may be written as a polynomial
\begin{equation}
A_{i,j} = a_{i,j} (H_{n_i}) J_{n_i,n_j} = J_{n_i,n_j} a_{i,j} (H_{n_j})
\end{equation}
where $a_{i,j}(H) ∈ k[H]/H^{n_j}$.
We simplify the notation and write~$A = (a_{i,j})$ where~$a_{i,j} ∈
k[H]$. We note however that elements of~$\ro C(M)$ do not multiply as
matrices with coefficients in~$k[H]$, due to the relations
of Prop.~\ref{prop:rel-HJ}. An easy way to perform the computations is
given in Prop.~\ref{prop:phantom} below.

\subsection{The adjunction involution}%<<<2

We now describe the involution~$A ↦ A^{⋆}$ of the commuting space~$\ro
C(M)$.

\begin{prop}\label{prop:big-T}
For each integer~$u$, write~$T_u$ for the anti-identity matrix of
size~$u$.
\begin{enumerate}
\item $T_u$~is invertible and both~$T_u$ and~$T_u H_u$ are symmetric.
\item $\transpose{J_{u,v}} T_u = T_v J_{v,u}$.
\item Let~$T$ be the block-diagonal matrix with diagonal blocks equal
to~$T_{n_i}$. Then $T ∈ \ro S(M)^{×}$.
\end{enumerate}
\end{prop}


\begin{prop}
Let~$A = (a_{i,j}(H)) ∈ \ro C(M)$. Then \[ A^{⋆} = (a_{ji}(H)). \]
In particular, $TA$~is symmetric if, and only if, $a_{i,j} = a_{ji}$
in~$k[H]/H^{\min (n_i, n_j)}$.
\end{prop}

\subsection{Structure of the commutant group}%<<<2


\begin{lem}\label{lem:det-1block}
Let~$s$ be such that~$n_1 = … = n_s > n_{s+1}$.
Define~$n'_i = n_i - 1$ if~$i ≤ s$, and~$n'_i = n_i$ if~$i > s$.

For any matrix~$A ∈ \ro C(M)$ given by the block decomposition~$A =
(a_{i,j}(H_{n_i}) J_{n_i,n_j})$, define~$A'$ as the square matrix of
size~$n - s$ given by the blocks~$(a_{i,j}(H_{n'_i}) J_{n'_i,
n'_j})$.

Then \[ \det A \;=\; \det (a_{i,j}(0))_{1 ≤ i,j ≤ s} · \det A'. \]
\end{lem}


\begin{proof}
Let~$σ$ the unique permutation of~$[1,n]$ such that~$σ(n_i) = i$ for~$i =
1, …, s$, and~$σ$ is increasing on all other indices. Then the matrix
$A^{σ}$ deduced from~$A$ by applying~$σ$ both on the lines and the
columns of~$A$ is lower block-triangular, with two diagonal blocks
respectively equal to the matrix~$(a_{i,j}(0))_{i,j ≤ s}$ and to~$A'$.
Since~$\det A = \det A^{σ}$, this proves the lemma.
\end{proof}

For each $u, v$, the \emph{big block} of index~$(u, v)$ of~$A$ is the
matrix~$(a_{i,j})$, where $i,j$~run over the range where~$n_i = u$
and~$n_j = v$. The next proposition states that $A$~is invertible if and
only if all its diagonal big blocks are invertible as matrices with
coefficients in~$k[H]/H^{n_i}$.

\begin{prop}\label{prop:det-bigblock}
Let~$A = (a_{i,j}(H)) ∈ \ro C(M)$. Then
\[ \det A \;=\; ∏_{d ≥ 1} \det (a_{i,j}(0) | n_{i} = n_j = d)^{d} \]
where $a_{i,j}(0)$~is the image modulo~$H$ of~$a_{i,j} ∈ k[H]$.
\end{prop}

\begin{proof}
By induction on~$n_1$, with Lemma~\ref{lem:det-1block} providing the
induction step. The base case~$n_1 = 1$ corresponds to~$k[H] = k$ and
therefore~$A = (a_{i,j}(0))$.
\end{proof}

\begin{prop}\label{prop:structure-gl}
The commutant group~$\ro C(M)^{×}$ is
generated by the following matrices:
\begin{enumerate}
\item big-block-diagonal matrices, i.e. matrices whose only non-zero big
blocks are those on the diagonal;
\item small-block transvection matrices.
\end{enumerate}
\end{prop}

\begin{proof}
By Prop.~\ref{prop:det-bigblock}, an invertible matrix~$A ∈ \ro C(M)$ has
all its diagonal big blocks invertible. Therefore we may apply the
Gaussian elimination algorithm to factor $A$~as a product~$A = LU$, where
$L$~is lower triangular with diagonal elements~$1$, and $U$~is upper
triangular.
\end{proof}

\section{Classification of local symmetric pencils}%<<<1
\subsection{Symmetric pencils as symmetric bilinear forms}%<<<2

As in the previous section, we assume that $M$~is a Jordan matrix
determined by integers~$n_1 ≥ … ≥ n_r$. Let~$R = k[H]/H^{n_1}$.

The next proposition gives an easy way to compute the congruence action
of the commutant group~$\ro C(M)^{×}$ on self-adjoint matrices, by way of
ordinary matrix multiplication in the matrix algebra~$R^{r×r}$.

\begin{prop}\label{prop:phantom}
For any self-adjoint matrix~$A ∈ \ro C(M)$, define the symmetric
matrix~$Φ(A)$ with coefficients in~$R$ by
\[ Φ(A) = (a_{i,j} H^{n_1 - n_j}) ∈ \ro S_r(R). \]
Likewise, for any invertible~$X ∈ \ro C(M)^{×}$, define
\[ Ψ(X) = (x_{i,j} H^{\max (n_1-n_j, 0)}) ∈ GL_r(R). \]
Then, for all~$A$ and~$X$, the following relation holds (as matrices with
coefficients in~$R$):
\[ Φ(X^{⋆}\:A\:X) = \transpose{Ψ(X)}\: Φ(A)\: Ψ(X) ∈ \ro S_r(R). \]
\end{prop}

\begin{proof}
We first note that the diagonal big blocks of~$Ψ(X)$ are equal to those
of~$X$, which are invertible. Since the big blocks above the diagonal
of~$Ψ(X)$ are all divisible by~$H$, this proves that $Ψ(X)$~is
invertible.

By Prop.~\ref{prop:structure-gl}, it is enough to check
Prop.~\ref{prop:phantom} when $X$~is big-block-diagonal or a transvection
matrix. The first case is obvious, and the second one
follows from the multiplication relations of the matrices~$J_{u,v}$
given in Prop.~\ref{prop:rel-HJ}~\itemref{it:Juw}.
\end{proof}

For example in the case of two big blocks: assume~$n_1 > n_2$, $e = n_1 -
n_2$, and let
\[ A = \mat{A_{11} & A_{12}\\ \transpose{A_{12}} & A_{22}}, \quad
X = \mat{X_{11} & X_{12}\\ X_{21} & X_{22}}. \]
According to Prop.~\ref{prop:phantom}, $Φ(A)$ and~$Ψ(X)$ are defined as
\[ Φ(A) = \mat{A_{11} & H^e A_{12}\\ H^e \transpose{A_{12}} & H^e A_{22}},
\quad
Ψ(X) = \mat{X_{11} & H^e X_{12}\\ X_{21} & X_{22}}. \]
and $X^{⋆} A X$~may be easily computed as the symmetric matrix such
that~$Φ(X^{⋆}AX) = \transpose{Ψ(X)} Φ(A) Ψ(X)$.

\subsection{Classical results on symmetric bilinear forms}%<<<2

In this paragraph, $R$~is the local ring~$R = k[H]/H^n$.
Let~$\acco{1, δ}$ be a set of representatives for the
quotient~$R^{×}/(R^{×})^2$.

\begin{prop}\label{prop:local-diag}
Let~$A$ be a regular symmetric matrix with entries in~$R$. Then $A$~is
congruent over~$R$ to either the identity matrix or the diagonal
matrix~$(1, …, 1, δ)$.
\end{prop}

\begin{proof}
The Gram orthogonalization algorithm works; cf.
\cite[I(3.4)]{milnorhusemoller} or \cite[92:1]{omeara}.
\end{proof}

\subsection{Classification of regular symmetric pencils}%<<<2


\begin{prop}\label{prop:bb-diag}
Let~$A$ be an invertible, self-adjoint element of~$\ro C(M)$. Then there
exists $X ∈ \ro C(M)^{×}$ such that $X^{*} A X$~is big-block-diagonal.
\end{prop}


\begin{proof}
We prove this by induction on the number of big blocks of~$A$, using Gram
orthogonalization on the big blocks of~$A$. Let~$B$ be the first diagonal
big block of~$A$ and~$e$ be the first non-zero element~$n_1 - n_i$. We
may then write $A$~as a block matrix
\begin{equation}
Φ(A) = \mat{B & H^{e} C\\ H^e\transpose{C} & H^e A'}.
\end{equation}
Since $A$~is invertible, all its diagonal big blocks are invertible; in
particular the matrices~$B$ and $A'$~are invertible. By the induction
hypothesis, there exists~$X'$ such that~$Δ = (X')^{*} A' X'$ is
big-block diagonal.

We then define
\begin{equation}
Ψ(X) = \mat{1 & -H^e B^{-1} C\\0 & X'}\quad\text{and see that}\quad
Φ(X^{⋆} A X) = \transpose{Ψ(X)} Φ(A) Ψ(X) = \mat{B & 0\\0 & H^e Δ}.
\end{equation}
\end{proof}

We note that the regularity hypothesis on~$A$ is essential for
Prop.~\ref{prop:bb-diag}. As a counter-example, assume that the Jordan
sequence is~$n_1 > n_2$ with~$n_2 ≥ 2$, and let~$A = \mat{H_{n_1} &
J_{n_1,n_2} \\ J_{n_2,n_1} & H_{n_2}} = \mat{H&1\\1&H}$.
Then no matrix of the form~$Ψ(X)$ diagonalizes~$Φ(A) = \mat{H&H\\H&H^2}$
in~$R^{2×2}$, and therefore $A$~is not not big-block diagonalizable by a
matrix commuting with~$M$.

\begin{thm}\label{thm:diag}
Let~$A$ be an invertible, self-adjoint element of~$\ro C(M)$. Then $A$~is
congruent to a diagonal matrix, where each diagonal big block is either
the identity matrix, or the diagonal matrix~$(1, …, 1, δ)$.
\end{thm}

\begin{proof}
Use propositions~\ref{prop:bb-diag} and~\ref{prop:local-diag}.
\end{proof}



% \section{In characteristic two}%<<<1
% \subsection{Action of the commutant on diagonal matrices}
% 
% Let~$u$ be an integer. We define a $k-$linear map~$λ_u$ from $u ×
% u$-matrices to~$R_u = k[H]/H^u$ in the following way:
% \begin{equation}
% λ_u((a_{i,j})_{i,j=1,…,u}) = ∑_{i=1}^{u} a_{i,i} H^{i-1}.
% \end{equation}
% 
% \begin{prop}
% Let~$φ: R → R$ be the Frobenius map, defined by~$φ(∑ x_i H^i) = ∑
% x_i^2 H^i$. Then for all~$x ∈ k[H_u]$,
% \begin{equation}
% λ_u (\transpose{x} \, A\, x) = φ(x)\, λ_u(A).
% \end{equation}
% \end{prop}
% 
% 
% \begin{proof}
% Given that $λ_u$~is $k$-linear, it is enough to prove that
% $λ_u(\transpose{H_u} \, A\, H_u) = H λ_u(A)$.
% \end{proof}
% 
% 
% \begin{lem}
% Let~$u, v$ be two integers and~$A ∈ R^{u × u}$.
% \begin{equation}
% λ_v ( \transpose{J_{u,v}}\, A\, J_{u,v}) \;=\;
% H^{\max (v-u, 0)}\,λ_u(A).
% \end{equation}
% \end{lem}
% 
% 
% \begin{prop}
% Let~$n_1 ≥ … ≥ n_r$. For any symmetric matrix~$A$ written as
% blocks~$A_{i,j}$ of size~$n_i × n_j$, define~$Λ(A)$ as the line matrix of
% length~$r$ with coefficients in~$R_1 = k[H]/H^{n_1}$:
% \begin{equation*}
% Λ(A) = ( H^{n_1 - n_i}\, λ_{n_i} (A_{i,i}))_{i=1,…, r}.
% \end{equation*}
% Then, for any~$X ∈ \ro C(M)$:
% \begin{equation}
% Λ(\transpose{X}\,A\,X) = Λ(A)\, φ(Ψ(X)).
% \end{equation}
% \end{prop}
% 
% 
% \begin{prop}
% Let~$A = \mat{Δ_1 & TM \\ 0 & Δ_2}$ and~$X = \mat{X_{1,1} & X_{1,2}\\
% X_{2,1} & X_{2,2}}$, where~$X_i ∈ \ro C(M)$ and~$Δ_i$ are diagonal.
% 
% Then $\transpose{X} A X$ has diagonal
% \[ Λ(\transpose{X} A X) = ( Λ(Δ_1) φΨ(X{1,i}) + Λ(Δ_2) φψ(X_{2,i}) +
%   Λ(\transpose{X_{1,i}} TM X_{2,i}))_{i=1,2}. \]
% \end{prop}
% 
\section{Recovering the second secret}

Two families of polynomials~$(a_1,…,a_m)$ and~$(b_1,…,b_m)$ are
\emph{isomorphic with two secrets} if there exist bijective linear
transformations~$s$ of the $n$~variables and~$h$ of the $m$~polynomials
such that $h ∘ a ∘ s = b$.

Assume that $m = 2$. Then the second secret~$t$ is a homography in two
variables. Assume moreover that $a_{λ} = λ a_∞ + a_0$ and~$b_{λ} = λ
b_{∞} + b_0$ are regular pencils of quadratic forms and that $2≠ 0$
in~$k$. Then the homography~$h$ maps the characteristic polynomial~$f(λ)
= \det (a_{λ})$ to~$g(λ) = \det (b_{λ})$. In particular, it maps the
prime factors of~$f$ to those of~$g$, respecting both their degree and
their exponent as a factor of the characteristic polynomial.

Arrange the factors of~$f$ and~$g$ by sets~$S_{i,j}$ and~$T_{i,j}$ of
factors of degree~$i$ and exponent~$j$. Then $h$~is the unique homography
mapping all the elements of~$S_{d,e}$ to~$T_{d,e}$ for each pair~$(d,e)$.
We compute the intersection for~$(d,e)$ of the set~$H_{d,e}$ of
homographies mapping the prime polynomials of~$S_{d,e}$ to~$T_{d,e}$. In
most cases, the first set~$H_{d,e}$ already has only one candidate, which
is therefore the second secret. The discussion depends on the degree~$d$
of the polynomials. We note that the sum of the size of the
sets~$S_{d,e}$ is the number of variables~$n$; therefore, we may use the
worst-case estimate~$\card{S_{d,e}} = O(n)$ for each~$(d,e)$.

We shall use the following classic results.
\begin{prop}\label{prop:homography}
\begin{enumerate}
\item Let~$(x_1, x_2, x_3)$ and~$(y_1, y_2, y_3)$ be two (ordered)
triples of distinct points of~$ℙ^1(k)$. There exists a unique
homography~$h ∈ \mathrm{PGL}_2(k)$ such that~$h(x_i) = y_i$.
\item Let~$(x_1, x_2, x_3, x_4)$ and~$(y_1, y_2, y_3, y_4)$ be two
(ordered) quadruplets of distinct points. They are homographic iff they
have the same cross-ratio~$B(x) = B(y)$, where
\begin{equation}
B(x) = \frac{(x_1-x_3)(x_2-x_4)}{(x_1-x_4)(x_2-x_3)}.
\end{equation}
\item Let~$\acco{x_1, x_2, x_3, x_4}$ and~$\acco{y_1, y_2, y_3, y_4}$ be
two (unordered) sets of four points. They are homographic iff they have
the same $j$-invariant~$j(x) = j(y)$, where
\begin{equation}
j(x) = \frac{(B(x)^2-B(x)+1)^3}{B(x)^2(1-B(x))^2}.
\end{equation}
\item Let~$u(x) = ∑ u_i x^i$ and~$v(x)$ be two monic polynomials
of degree four. They are homographic iff they have the same $j$-invariant,
where $j(u)$~is a polynomial of degree~$6$ in the coefficients of~$u$.
\end{enumerate}
\end{prop}


\paragraph{Case~$d = 1$.}
If $\card{S_{1,e}} ≥ 3$, then we may immediately recover the
homography~$h$: namely, fix a triple~$(x_1,x_2,x_3)$ in~$S_{1,e}$, and
iterate over the triples in~$T_{1,e}$. For each such triple, there exists
a unique homography~$h$ such that~$h(x_i) = y_i$. This homography belongs
to~$H_{1,e}$ iff the images of all the other points of~$S_{1,e}$ belong
to~$T_{1,e}$. Since there are~$3!\binom{\card{S_{1,e}}}{3} = O(n^3)$
triples~$(y_i)$, this computation requires~$O(n^3)$ field operations.

If $1 ≤ \card{S_{1,e}} ≤ 2$, then $H_{1,e}$~may be explicitly computed as
the union of the set of homographies mapping the elements of~$S_{1,e}$ to
those of~$T_{1,e}$ for all permutations of~$T_{1,e}$.

\paragraph{Case~$d = 2$.}
Assume $\card{S_{2,e}} ≥ 2$. Let~$u_1, u_2 ∈ S_{2,e}$ and~$v_1, v_2 ∈
T_{2,e}$ be monic polynomials of degree~two. Any homography between the
sets~$\acco{u_1, u_2}$ and~$\acco{v_1, v_2}$ will map~$u_1 u_2$ to~$v_1
v_2$. By Prop.~\ref{prop:homography}(iv), there exists at most a bounded
number of such homographies. Since there are~$\binom{\card{S_{2,e}}}{2} =
O(n^2)$ pairs~$(v_1, v_2)$, this requires~$O(n^2)$ field operations.

If~$\card{S_{2,e}} = 1$, then $H_{2,e}$~is the set of all homographies
mapping the unique element of~$S_{2,e}$ to the unique element
of~$T_{2,e}$.

\paragraph{Case~$d = 3$.}
Fix an element~$u ∈ S_{3,e}$. For all~$v ∈ T_{3,e}$, there exist at
most~$3! = 6$ homographies~$h$ mapping~$u$ to~$v$. Each candidate belongs
to~$H_{3,e}$ iff it maps all other elements of~$S_{3,e}$ to elements
of~$T_{3,e}$. There are~$\card{S_{3,e}} = O(n)$ candidates~$u$ and
therefore~$O(n)$ candidate homographies~$h$.

\paragraph{Case~$d = 4$.}
Fix an element~$u ∈ S_{4,e}$. The candidates as homographic images of~$u$
in~$T_{4,e}$ are the~$v$ such that~$j(v) = j(u)$. Each candidate
polynomial~$v$ gives at most $4! = 24$~candidates homographies~$h$. This
allows to compute~$H_{4,e}$ in~$O(n)$ field operations.

\paragraph{Case~$d ≥ 5$.} The naïve method is to derive~$(d-4)$ times the
elements of~$S_{d,e}$ to reduce to the case where~$d = 4$. However, as
this uses only the five leading coefficients, if the polynomials are
specially chosen we may find too many homographies. Instead, we first
compose all the elements of~$S_{d,e}$ and~$T_{d,e}$ by a known, randomly
chosen homography~$r$. If it still happens that there are two
non-homographic elements~$u_1, u_2 ∈ S_{d,e}$ such that~$(r ∘
u_i)^{(d-4)}$ are homographic, we only need to change the random
homography~$r$. In this way, we may compute the set~$H_{d,e}$ in at
most~$O(n)$ field operations.

\paragraph{Computing the hidden homography.}

The hidden homography~$h$ lies in the intersection of all sets~$H_{d,e}$.
As each one of these sets is likely to be extremely small or even reduced
to~$\acco{h}$, we compute them in increasing order of assumed complexity,
using estimates given above depending on~$d$ and~$\card{S_{d,e}}$. In
total, we find a bounded number of candidate homographies using no more
than~$O(n^3)$ operations in~$k$.

\section{Algorithms and Complexity}
Voir avec Jerome ce qu'on met ici en fonction des connaissances ou si on le met a la fin de chaque section


\end{document}
