\documentclass{article}%<<<
\usepackage[margin=20mm]{geometry}
\usepackage{unicode}
\usepackage{amsmath,amssymb,amsthm,mathrsfs}

\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\def\labelenumi{(\roman{enumi})}
\def\itemref#1{\expandafter\ifx\csname r@#1\endcsname\relax
  {\bfseries ??}\else{\setcounter{enumi}{\ref{#1}}\labelenumi}\fi}
\def\linkcounter#1#2{\edef\magic{\noexpand\let
  \expandafter\noexpand\csname c@#1\endcsname
  \expandafter\noexpand\csname c@#2\endcsname}\magic}
\linkcounter{thm}{prop}
\linkcounter{lem}{prop}
\linkcounter{equation}{prop}

\def\bigperp{\mathop{\vcenter{\hbox{\scalebox{2}{\ensuremath{\perp}}}}}%
  \displaylimits}
\let\fr\mathfrak
\let\ro\mathscr
\def\transpose{\,{}^{\mathrm{t}\!}}
\def\acco#1{\left\{#1\right\}}
\def\abs#1{\left|#1\right|}
\def\mat#1{\begin{pmatrix}#1\end{pmatrix}}
\def\card#1{\abs{#1}}
%>>>

\begin{document}
\title{Solving the ``isomorphism of polynomials'' problem with pencils of
bilinear forms}
\author{Pierre-Alain Fouque \and Gilles Macario-Rat \and Jérôme Plût}
\maketitle

\section{Local structure of symmetric pencils}%<<<1
\subsection{From symmetric to commuting matrices}%<<<2

Let~$k$ be a field. A \emph{symmetric pencil} is an affine line in the
space of symmetric bilinear forms with coefficients in~$k$:
\begin{equation}
b_{λ} = λ S_{∞} + S_0.
\end{equation}
Up to a change of the basis~$(S_{∞}, S_{0})$ of~$S$, we may assume that
$S_{∞}$ divides~$S_0$ on the left. Defining~$M = S_{∞}^{-1} S_0^{}$, we
may therefore write the pencil~$(S_{λ})$ as
\begin{equation}
S_{λ} = S_{∞} (λ + M),
\end{equation}
where $S_{∞}$ and~$S_{∞} M$ are symmetric matrices. We say that
$(S_{λ})$ is \emph{regular} if $S_{∞}$~is invertible. The matrix $M$~is the
\emph{characteristic endomorphism} of the pencil~$(S_{λ})$.

The image of the pencil~$(S_{λ})$ by a linear change of basis~$X$
is~$(S'_{λ}) = (\transpose{X} S_{λ} X)$; its characteristic endomorphism
is~$M' = X^{-1} M X$. In particular, $M = M'$ exactly when~$X$ and~$M$
commute.


For any matrix~$M$, we define the \emph{symmetrizing space}~$\ro S(M)$
and the \emph{commutant}~$\ro C(M)$ as
\begin{gather}
\ro S(M) = \acco{\text{$S$ symmetric such that $SM$~is symmetric} },\\
\ro C(M) = \text{\text{$X$ such that~$XM = MX$}}.
\end{gather}
The invertible elements of~$\ro C(M)$ form the \emph{commutant
group}~$\ro C(M)^{×}$.

In the particular case where $M$~is cyclic, the commutant~$\ro C(M)$ is
the (commutative) polynomial algebra~$k[M]$. In this case, the IP1S
problem may be solved in a straightforward way~\cite{MPG2013}. We are
interested here in the general case, where $\ro C(M)$~is usually not a
commutative algebra.

\begin{prop}\label{prop:structure-sym}
Let~$M$ be any matrix.
\begin{enumerate}
\item \label{it:sym-inv} The set~$\ro S(M)$ contains an invertible
matrix~$T$.
\item \label{it:sym-comm} For any invertible~$T ∈ \ro S(M)$ and any~$A$,
the matrix $TA$~belongs to~$\ro S(M)$ if and only if $TA$~is symmetric
and $A ∈ \ro C(M)$.
\item Any symmetric pencil with characteristic endomorphism~$M$ is of the
form~$S_{λ} = TA (λ + M)$ where $A ∈ \ro C(M)$. Moreover, it is regular
if and only if $A$~is invertible.
\item Let~$S_{λ} = TA (λ + M)$ be a symmetric pencil and~$X ∈ \ro C(M)$.
Then
\begin{equation*}
\transpose{X} S_{λ} X = T \:(X^{⋆} A X)\: (λ + M), \quad
\text{where $X^{⋆} = T^{-1} \transpose{X} T ∈ \ro C(M)$.}
\end{equation*}
\item Let~$A ∈ \ro C(M)$. Then $\transpose{(TA)} = TA^{⋆}$. In
particular, $TA$~is symmetric if, and only if, $A = A^{⋆}$.
\end{enumerate}
\end{prop}

\begin{proof}
Point~\itemref{it:sym-inv} is explicitly proven in Prop.~\ref{prop:big-T}
below. Assuming that $TA$~is symmetric, point~\itemref{it:sym-comm}
follows from
\begin{equation}
\transpose{(TAM)} \;=\; \transpose{M}\, \transpose{(TA)}
 \;=\; \transpose{M}\,TA = TMA;
\end{equation}
since $T$~is invertible, it is cancellable in the resulting equation~$TMA
= TAM$.
\end{proof}

We call~$A^{⋆}$ the \emph{adjoint} of~$A$ and say that $A$~is
\emph{self-adjoint} if~$A^{⋆} = A$. (Since we don't plan to equip the
field~$k$ with an involution, no confusion with the usual meaning is
possible).

\begin{thm}\label{thm:IP1S-congruence}
The congruence problem for regular symmetric pencils is equivalent to the
following: given two invertible self-adjoint matrices~$A, A' ∈ \ro C(M)$,
compute a matrix~$X ∈ \ro C(M)$ such that~$X^{⋆} A X = A'$.
\end{thm}

\subsection{The commuting space of a Jordan matrix}%<<<2

By using the primary decomposition of~$M$ (= Chinese remainders) as well
as an extension of scalars, we may assume that the minimal polynomial
of~$M$ is a power of a prime polynomial. Using an extension of scalars, we
assume that it is a power of~$x$.

In this case, $M$~decomposes as Jordan blocks as follows: for each
integer~$u$, let~$H_u$ be the companion matrix of the polynomial~$x^u$.
Then there exists a decreasing sequence of integers~$n_1 ≥ … ≥ n_r$ such
that~$∑ n_i = n$ and $M$~is block-diagonal, with the $i$-th block being
equal to~$H_{n_i}$.

For each integers~$u ≥ v$, define matrices~$J_{u,v}$ and~$J_{v,u}$, of
respective sizes~$u × v$ and~$v × u$, by the block decompositions:
\begin{equation}
J_{u,v} = \mat{ \mathrm{id}(v) \\ 0^{v ×(u-v)}},\quad
J_{v,u} = \mat{ 0^{(u-v)× v } & \mathrm{id}(v)}.
\end{equation}

\begin{prop}\label{prop:rel-HJ}
Let~$u, v, w$ be three integers.
\begin{enumerate}
\item $H_{u} J_{u,v} = J_{u,v} H_{v}$.
\item The space of all matrices~$A$ of size~$u × v$ such that~$H_u A = A
H_v$ is exactly $k[H_u] J_{u,v}$.
\item \label{it:Juw} $J_{u,v} J_{v,w} = H_v^{d} J_{v,w}^{}$ where $d$~is
the distance between~$v$ and the interval~$[u,w]$. In particular,
$J_{u,v} J_{v,u} = H_{u}^{\abs{u-v}}$.
\end{enumerate}
\end{prop}

\begin{prop}\label{prop:structure-commutant}
Let~$M$ be the block-diagonal matrix with diagonal blocks~$H_{n_i}$.
The commuting space of~$M$ is the space of
all block matrices~$A = (A_{i,j})$, where $A_{i,j}$~belongs
to~$k[H_{n_i}] J_{n_{i}, n_{j}} = J_{n_i, n_j} k[H_{n_j}]$.
\end{prop}

Each entry~$A_{i,j}$ may be written as a polynomial
\begin{equation}
A_{i,j} = a_{i,j} (H_{n_i}) J_{n_i,n_j} = J_{n_i,n_j} a_{i,j} (H_{n_j})
\end{equation}
where $a_{i,j}(H) ∈ k[H]/H^{n_j}$.
We simplify the notation and write~$A = (a_{i,j})$ where~$a_{i,j} ∈
k[H]$. We note however that elements of~$\ro C(M)$ do not multiply as
matrices with coefficients in~$k[H]$, due to the relations
of Prop.~\ref{prop:rel-HJ}. An easy way to perform the computations is
given in Prop.~\ref{prop:phantom} below.

\subsection{The adjunction involution}%<<<2

We now describe the involution~$A ↦ A^{⋆}$ of the commuting space~$\ro
C(M)$.

\begin{prop}\label{prop:big-T}
For each integer~$u$, write~$T_u$ for the anti-identity matrix of
size~$u$.
\begin{enumerate}
\item $T_u$~is invertible and both~$T_u$ and~$T_u H_u$ are symmetric.
\item $\transpose{J_{u,v}} T_u = T_v J_{v,u}$.
\item Let~$T$ be the block-diagonal matrix with diagonal blocks equal
to~$T_{n_i}$. Then $T ∈ \ro S(M)^{×}$.
\end{enumerate}
\end{prop}


\begin{prop}
Let~$A = (a_{i,j}(H)) ∈ \ro C(M)$. Then \[ A^{⋆} = (a_{ji}(H)). \]
In particular, $TA$~is symmetric if, and only if, $a_{i,j} = a_{ji}$
in~$k[H]/H^{\min (n_i, n_j)}$.
\end{prop}

\subsection{Structure of the commutant group}%<<<2


\begin{lem}\label{lem:det-1block}
Let~$s$ be such that~$n_1 = … = n_s > n_{s+1}$.
Define~$n'_i = n_i - 1$ if~$i ≤ s$, and~$n'_i = n_i$ if~$i > s$.

For any matrix~$A ∈ \ro C(M)$ given by the block decomposition~$A =
(a_{i,j}(H_{n_i}) J_{n_i,n_j})$, define~$A'$ as the square matrix of
size~$n - s$ given by the blocks~$(a_{i,j}(H_{n'_i}) J_{n'_i,
n'_j})$.

Then \[ \det A \;=\; \det (a_{i,j}(0))_{1 ≤ i,j ≤ s} · \det A'. \]
\end{lem}


\begin{proof}
Let~$σ$ the unique permutation of~$[1,n]$ such that~$σ(n_i) = i$ for~$i =
1, …, s$, and~$σ$ is increasing on all other indices. Then the matrix
$A^{σ}$ deduced from~$A$ by applying~$σ$ both on the lines and the
columns of~$A$ is lower block-triangular, with two diagonal blocks
respectively equal to the matrix~$(a_{i,j}(0))_{i,j ≤ s}$ and to~$A'$.
Since~$\det A = \det A^{σ}$, this proves the lemma.
\end{proof}

For each $u, v$, the \emph{big block} of index~$(u, v)$ of~$A$ is the
matrix~$(a_{i,j})$, where $i,j$~run over the range where~$n_i = u$
and~$n_j = v$. The next proposition states that $A$~is invertible if and
only if all its diagonal big blocks are invertible as matrices with
coefficients in~$k[H]/H^{n_i}$.

\begin{prop}\label{prop:det-bigblock}
Let~$A = (a_{i,j}(H)) ∈ \ro C(M)$. Then
\[ \det A \;=\; ∏_{d ≥ 1} \det (a_{i,j}(0) | n_{i} = n_j = d)^{d} \]
where $a_{i,j}(0)$~is the image modulo~$H$ of~$a_{i,j} ∈ k[H]$.
\end{prop}

\begin{proof}
By induction on~$n_1$, with Lemma~\ref{lem:det-1block} providing the
induction step. The base case~$n_1 = 1$ corresponds to~$k[H] = k$ and
therefore~$A = (a_{i,j}(0))$.
\end{proof}

\begin{prop}\label{prop:structure-gl}
The commutant group~$\ro C(M)^{×}$ is
generated by the following matrices:
\begin{enumerate}
\item big-block-diagonal matrices, i.e. matrices whose only non-zero big
blocks are those on the diagonal;
\item small-block transvection matrices.
\end{enumerate}
\end{prop}

\begin{proof}
By Prop.~\ref{prop:det-bigblock}, an invertible matrix~$A ∈ \ro C(M)$ has
all its diagonal big blocks invertible. Therefore we may apply the
Gaussian elimination algorithm to factor $A$~as a product~$A = LU$, where
$L$~is lower triangular with diagonal elements~$1$, and $U$~is upper
triangular.
\end{proof}

\section{Classification of local symmetric pencils}%<<<1
\subsection{Symmetric pencils as symmetric bilinear forms}%<<<2

As in the previous section, we assume that $M$~is a Jordan matrix
determined by integers~$n_1 ≥ … ≥ n_r$. Let~$R = k[H]/H^{n_1}$.

The next proposition gives an easy way to compute the congruence action
of the commutant group~$\ro C(M)^{×}$ on self-adjoint matrices, by way of
ordinary matrix multiplication in the matrix algebra~$R^{r×r}$.

\begin{prop}\label{prop:phantom}
For any self-adjoint matrix~$A ∈ \ro C(M)$, define the symmetric
matrix~$Φ(A)$ with coefficients in~$R$ by
\[ Φ(A) = (a_{i,j} H^{n_1 - n_j}) ∈ \ro S_r(R). \]
Likewise, for any invertible~$X ∈ \ro C(M)^{×}$, define
\[ Ψ(X) = (x_{i,j} H^{\max (n_1-n_j, 0)}) ∈ GL_r(R). \]
Then, for all~$A$ and~$X$, the following relation holds (as matrices with
coefficients in~$R$):
\[ Φ(X^{⋆}\:A\:X) = \transpose{Ψ(X)}\: Φ(A)\: Ψ(X) ∈ \ro S_r(R). \]
\end{prop}

\begin{proof}
We first note that the diagonal big blocks of~$Ψ(X)$ are equal to those
of~$X$, which are invertible. Since the big blocks above the diagonal
of~$Ψ(X)$ are all divisible by~$H$, this proves that $Ψ(X)$~is
invertible.

By Prop.~\ref{prop:structure-gl}, it is enough to check
Prop.~\ref{prop:phantom} when $X$~is big-block-diagonal or a transvection
matrix. The first case is obvious, and the second one
follows from the multiplication relations of the matrices~$J_{u,v}$
given in Prop.~\ref{prop:rel-HJ}~\itemref{it:Juw}.
\end{proof}

For example in the case of two big blocks: assume~$n_1 > n_2$, $e = n_1 -
n_2$, and let
\[ A = \mat{A_{11} & A_{12}\\ \transpose{A_{12}} & A_{22}}, \quad
X = \mat{X_{11} & X_{12}\\ X_{21} & X_{22}}. \]
According to Prop.~\ref{prop:phantom}, $Φ(A)$ and~$Ψ(X)$ are defined as
\[ Φ(A) = \mat{A_{11} & H^e A_{12}\\ H^e \transpose{A_{12}} & H^e A_{22}},
\quad
Ψ(X) = \mat{X_{11} & H^e X_{12}\\ X_{21} & X_{22}}. \]
and $X^{⋆} A X$~may be easily computed as the symmetric matrix such
that~$Φ(X^{⋆}AX) = \transpose{Ψ(X)} Φ(A) Ψ(X)$.

\subsection{Classical results on symmetric bilinear forms}%<<<2

In this paragraph, $R$~is the local ring~$R = k[H]/H^n$.
Let~$\acco{1, δ}$ be a set of representatives for the
quotient~$R^{×}/(R^{×})^2$.

\begin{prop}\label{prop:local-diag}
Let~$A$ be a regular symmetric matrix with entries in~$R$. Then $A$~is
congruent over~$R$ to either the identity matrix or the diagonal
matrix~$(1, …, 1, δ)$.
\end{prop}

\begin{proof}
The Gram orthogonalization algorithm works; cf.
\cite[I(3.4)]{milnorhusemoller} or \cite[92:1]{omeara}.
\end{proof}

\subsection{Classification of regular symmetric pencils}%<<<2


\begin{prop}\label{prop:bb-diag}
Let~$A$ be an invertible, self-adjoint element of~$\ro C(M)$. Then there
exists $X ∈ \ro C(M)^{×}$ such that $X^{*} A X$~is big-block-diagonal.
\end{prop}


\begin{proof}
We prove this by induction on the number of big blocks of~$A$, using Gram
orthogonalization on the big blocks of~$A$. Let~$B$ be the first diagonal
big block of~$A$ and~$e$ be the first non-zero element~$n_1 - n_i$. We
may then write $A$~as a block matrix
\begin{equation}
Φ(A) = \mat{B & H^{e} C\\ H^e\transpose{C} & H^e A'}.
\end{equation}
Since $A$~is invertible, all its diagonal big blocks are invertible; in
particular the matrices~$B$ and $A'$~are invertible. By the induction
hypothesis, there exists~$X'$ such that~$Δ = (X')^{*} A' X'$ is
big-block diagonal.

We then define
\begin{equation}
Ψ(X) = \mat{1 & -H^e B^{-1} C\\0 & X'}\quad\text{and see that}\quad
Φ(X^{⋆} A X) = \transpose{Ψ(X)} Φ(A) Ψ(X) = \mat{B & 0\\0 & H^e Δ}.
\end{equation}
\end{proof}

We note that the regularity hypothesis on~$A$ is essential for
Prop.~\ref{prop:bb-diag}. As a counter-example, assume that the Jordan
sequence is~$n_1 > n_2$ with~$n_2 ≥ 2$, and let~$A = \mat{H_{n_1} &
J_{n_1,n_2} \\ J_{n_2,n_1} & H_{n_2}} = \mat{H&1\\1&H}$.
Then no matrix of the form~$Ψ(X)$ diagonalizes~$Φ(A) = \mat{H&H\\H&H^2}$
in~$R^{2×2}$, and therefore $A$~is not not big-block diagonalizable by a
matrix commuting with~$M$.

\begin{thm}\label{thm:diag}
Let~$A$ be an invertible, self-adjoint element of~$\ro C(M)$. Then $A$~is
congruent to a diagonal matrix, where each diagonal big block is either
the identity matrix, or the diagonal matrix~$(1, …, 1, δ)$.
\end{thm}

\begin{proof}
Use propositions~\ref{prop:bb-diag} and~\ref{prop:local-diag}.
\end{proof}


% \section{In characteristic two}%<<<1
% \subsection{Action of the commutant on diagonal matrices}
% 
% Let~$u$ be an integer. We define a $k-$linear map~$λ_u$ from $u ×
% u$-matrices to~$R_u = k[H]/H^u$ in the following way:
% \begin{equation}
% λ_u((a_{i,j})_{i,j=1,…,u}) = ∑_{i=1}^{u} a_{i,i} H^{i-1}.
% \end{equation}
% 
% \begin{prop}
% Let~$φ: R → R$ be the Frobenius map, defined by~$φ(∑ x_i H^i) = ∑
% x_i^2 H^i$. Then for all~$x ∈ k[H_u]$,
% \begin{equation}
% λ_u (\transpose{x} \, A\, x) = φ(x)\, λ_u(A).
% \end{equation}
% \end{prop}
% 
% 
% \begin{proof}
% Given that $λ_u$~is $k$-linear, it is enough to prove that
% $λ_u(\transpose{H_u} \, A\, H_u) = H λ_u(A)$.
% \end{proof}
% 
% 
% \begin{lem}
% Let~$u, v$ be two integers and~$A ∈ R^{u × u}$.
% \begin{equation}
% λ_v ( \transpose{J_{u,v}}\, A\, J_{u,v}) \;=\;
% H^{\max (v-u, 0)}\,λ_u(A).
% \end{equation}
% \end{lem}
% 
% 
% \begin{prop}
% Let~$n_1 ≥ … ≥ n_r$. For any symmetric matrix~$A$ written as
% blocks~$A_{i,j}$ of size~$n_i × n_j$, define~$Λ(A)$ as the line matrix of
% length~$r$ with coefficients in~$R_1 = k[H]/H^{n_1}$:
% \begin{equation*}
% Λ(A) = ( H^{n_1 - n_i}\, λ_{n_i} (A_{i,i}))_{i=1,…, r}.
% \end{equation*}
% Then, for any~$X ∈ \ro C(M)$:
% \begin{equation}
% Λ(\transpose{X}\,A\,X) = Λ(A)\, φ(Ψ(X)).
% \end{equation}
% \end{prop}
% 
% 
% \begin{prop}
% Let~$A = \mat{Δ_1 & TM \\ 0 & Δ_2}$ and~$X = \mat{X_{1,1} & X_{1,2}\\
% X_{2,1} & X_{2,2}}$, where~$X_i ∈ \ro C(M)$ and~$Δ_i$ are diagonal.
% 
% Then $\transpose{X} A X$ has diagonal
% \[ Λ(\transpose{X} A X) = ( Λ(Δ_1) φΨ(X{1,i}) + Λ(Δ_2) φψ(X_{2,i}) +
%   Λ(\transpose{X_{1,i}} TM X_{2,i}))_{i=1,2}. \]
% \end{prop}
% 
\section{Recovering the second secret}

Two families of polynomials~$(a_1,…,a_m)$ and~$(b_1,…,b_m)$ are
\emph{isomorphic with two secrets} if there exist bijective linear
transformations~$s$ of the $n$~variables and~$h$ of the $m$~polynomials
such that $h ∘ a ∘ s = b$.

Assume that $m = 2$. Then the second secret~$t$ is a homography in two
variables. Assume moreover that $a_{λ} = λ a_∞ + a_0$ and~$b_{λ} = λ
b_{∞} + b_0$ are regular pencils of quadratic forms and that $2≠ 0$
in~$k$. Then the homography~$h$ maps the characteristic polynomial~$f(λ)
= \det (a_{λ})$ to~$g(λ) = \det (b_{λ})$. In particular, it maps the
prime factors of~$f$ to those of~$g$, respecting both their degree and
their exponent as a factor of the characteristic polynomial.

Arrange the factors of~$f$ and~$g$ by sets~$S_{i,j}$ and~$T_{i,j}$ of
factors of degree~$i$ and exponent~$j$. Then $h$~is the unique homography
mapping all the elements of~$S_{d,e}$ to~$T_{d,e}$ for each pair~$(d,e)$.
We compute the intersection for~$(d,e)$ of the set~$H_{d,e}$ of
homographies mapping the prime polynomials of~$S_{d,e}$ to~$T_{d,e}$. In
most cases, the first set~$H_{d,e}$ already has only one candidate, which
is therefore the second secret. The discussion depends on the degree~$d$
of the polynomials. We note that the sum of the size of the
sets~$S_{d,e}$ is the number of variables~$n$; therefore, we may use the
worst-case estimate~$\card{S_{d,e}} = O(n)$ for each~$(d,e)$.

We shall use the following classic results.
\begin{prop}\label{prop:homography}
\begin{enumerate}
\item Let~$(x_1, x_2, x_3)$ and~$(y_1, y_2, y_3)$ be two (ordered)
triples of distinct points of~$ℙ^1(k)$. There exists a unique
homography~$h ∈ \mathrm{PGL}_2(k)$ such that~$h(x_i) = y_i$.
\item Let~$(x_1, x_2, x_3, x_4)$ and~$(y_1, y_2, y_3, y_4)$ be two
(ordered) quadruplets of distinct points. They are homographic iff they
have the same cross-ratio~$B(x) = B(y)$, where
\begin{equation}
B(x) = \frac{(x_1-x_3)(x_2-x_4)}{(x_1-x_4)(x_2-x_3)}.
\end{equation}
\item Let~$\acco{x_1, x_2, x_3, x_4}$ and~$\acco{y_1, y_2, y_3, y_4}$ be
two (unordered) sets of four points. They are homographic iff they have
the same $j$-invariant~$j(x) = j(y)$, where
\begin{equation}
j(x) = \frac{(B(x)^2-B(x)+1)^3}{B(x)^2(1-B(x))^2}.
\end{equation}
\item Let~$u(x) = ∑ u_i x^i$ and~$v(x)$ be two monic polynomials
of degree four. They are homographic iff they have the same $j$-invariant,
where $j(u)$~is a polynomial of degree~$6$ in the coefficients of~$u$.
\end{enumerate}
\end{prop}


\paragraph{Case~$d = 1$.}
If $\card{S_{1,e}} ≥ 3$, then we may immediately recover the
homography~$h$: namely, fix a triple~$(x_1,x_2,x_3)$ in~$S_{1,e}$, and
iterate over the triples in~$T_{1,e}$. For each such triple, there exists
a unique homography~$h$ such that~$h(x_i) = y_i$. This homography belongs
to~$H_{1,e}$ iff the images of all the other points of~$S_{1,e}$ belong
to~$T_{1,e}$. Since there are~$3!\binom{\card{S_{1,e}}}{3} = O(n^3)$
triples~$(y_i)$, this computation requires~$O(n^3)$ field operations.

If $1 ≤ \card{S_{1,e}} ≤ 2$, then $H_{1,e}$~may be explicitly computed as
the union of the set of homographies mapping the elements of~$S_{1,e}$ to
those of~$T_{1,e}$ for all permutations of~$T_{1,e}$.

\paragraph{Case~$d = 2$.}
Assume $\card{S_{2,e}} ≥ 2$. Let~$u_1, u_2 ∈ S_{2,e}$ and~$v_1, v_2 ∈
T_{2,e}$ be monic polynomials of degree~two. Any homography between the
sets~$\acco{u_1, u_2}$ and~$\acco{v_1, v_2}$ will map~$u_1 u_2$ to~$v_1
v_2$. By Prop.~\ref{prop:homography}(iv), there exists at most a bounded
number of such homographies. Since there are~$\binom{\card{S_{2,e}}}{2} =
O(n^2)$ pairs~$(v_1, v_2)$, this requires~$O(n^2)$ field operations.

If~$\card{S_{2,e}} = 1$, then $H_{2,e}$~is the set of all homographies
mapping the unique element of~$S_{2,e}$ to the unique element
of~$T_{2,e}$.

\paragraph{Case~$d = 3$.}
Fix an element~$u ∈ S_{3,e}$. For all~$v ∈ T_{3,e}$, there exist at
most~$3! = 6$ homographies~$h$ mapping~$u$ to~$v$. Each candidate belongs
to~$H_{3,e}$ iff it maps all other elements of~$S_{3,e}$ to elements
of~$T_{3,e}$. There are~$\card{S_{3,e}} = O(n)$ candidates~$u$ and
therefore~$O(n)$ candidate homographies~$h$.

\paragraph{Case~$d = 4$.}
Fix an element~$u ∈ S_{4,e}$. The candidates as homographic images of~$u$
in~$T_{4,e}$ are the~$v$ such that~$j(v) = j(u)$. Each candidate
polynomial~$v$ gives at most $4! = 24$~candidates homographies~$h$. This
allows to compute~$H_{4,e}$ in~$O(n)$ field operations.

\paragraph{Case~$d ≥ 5$.} The naïve method is to derive~$(d-4)$ times the
elements of~$S_{d,e}$ to reduce to the case where~$d = 4$. However, as
this uses only the five leading coefficients, if the polynomials are
specially chosen we may find too many homographies. Instead, we first
compose all the elements of~$S_{d,e}$ and~$T_{d,e}$ by a known, randomly
chosen homography~$r$. If it still happens that there are two
non-homographic elements~$u_1, u_2 ∈ S_{d,e}$ such that~$(r ∘
u_i)^{(d-4)}$ are homographic, we only need to change the random
homography~$r$. In this way, we may compute the set~$H_{d,e}$ in at
most~$O(n)$ field operations.

\paragraph{Computing the hidden homography.}

The hidden homography~$h$ lies in the intersection of all sets~$H_{d,e}$.
As each one of these sets is likely to be extremely small or even reduced
to~$\acco{h}$, we compute them in increasing order of assumed complexity,
using estimates given above depending on~$d$ and~$\card{S_{d,e}}$. In
total, we find a bounded number of candidate homographies using no more
than~$O(n^3)$ operations in~$k$.


\begin{thebibliography}{99}%<<<1
\bibitem{omeara} Timothy O'Meara, \emph{Introduction to quadratic forms}.
\bibitem{milnorhusemoller} H. Milnor, D. Husemoller, \emph{Symmetric
bilinear forms}. \bibitem{MPG2013} G. Macario-Rat, J. Plût, H. Gilbert,
\emph{New Insight into the Isomorphism of Polynomial Problem IP1S and its
Use in Cryptography}, ASIACRYPT 2013.
\end{thebibliography}%>>>1
\end{document}
