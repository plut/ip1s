\documentclass{article}%<<<
\usepackage[margin=20mm]{geometry}
\usepackage{unicode}
\usepackage{amsmath,amssymb,amsthm,mathrsfs}

\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\def\labelenumi{(\roman{enumi})}
\def\itemref#1{\expandafter\ifx\csname r@#1\endcsname\relax
  {\bfseries ??}\else{\setcounter{enumi}{\ref{#1}}\labelenumi}\fi}
\def\linkcounter#1#2{\edef\magic{\noexpand\let
  \expandafter\noexpand\csname c@#1\endcsname
  \expandafter\noexpand\csname c@#2\endcsname}\magic}
\linkcounter{thm}{prop}
\linkcounter{lem}{prop}
\linkcounter{equation}{prop}

\def\bigperp{\mathop{\vcenter{\hbox{\scalebox{2}{\ensuremath{\perp}}}}}%
  \displaylimits}
\let\fr\mathfrak
\let\ro\mathscr
\def\transpose{\,{}^{\mathrm{t}\!}}
\def\acco#1{\left\{#1\right\}}
\def\abs#1{\left|#1\right|}
\def\mat#1{\begin{pmatrix}#1\end{pmatrix}}
%>>>

\begin{document}
\title{Solving the ``isomorphism of polynomials'' problem with pencils of
bilinear forms}
\author{Pierre-Alain Fouque \and Gilles Macario-Rat \and Jérôme Plût}
\maketitle

\section{Local structure of symmetric pencils}%<<<1
\subsection{From symmetric to commuting matrices}%<<<2

Let~$k$ be a field. A \emph{symmetric pencil} is an affine line in the
space of symmetric bilinear forms with coefficients in~$k$:
\begin{equation}
b_{λ} = λ S_{∞} + S_0.
\end{equation}
Up to a change of the basis~$(S_{∞}, S_{0})$ of~$S$, we may assume that
$S_{∞}$ divides~$S_0$ on the left. Defining~$M = S_{∞}^{-1} S_0^{}$, we
may therefore write the pencil~$(S_{λ})$ as
\begin{equation}
S_{λ} = S_{∞} (λ + M),
\end{equation}
where $S_{∞}$ and~$S_{∞} M$ are symmetric matrices. We say that
$(S_{λ})$ is \emph{regular} if $S_{∞}$~is invertible. The matrix $M$~is the
\emph{characteristic endomorphism} of the pencil~$(S_{λ})$.

The image of the pencil~$(S_{λ})$ by a linear change of basis~$X$
is~$(S'_{λ}) = (\transpose{X} S_{λ} X)$; its characteristic endomorphism
is~$M' = X^{-1} M X$. In particular, $M = M'$ exactly when~$X$ and~$M$
commute.


For any matrix~$M$, we define the \emph{symmetrizing space}~$\ro S(M)$
and the \emph{commutant}~$\ro C(M)$ as
\begin{gather}
\ro S(M) = \acco{\text{$S$ symmetric such that $SM$~is symmetric} },\\
\ro C(M) = \text{\text{$X$ such that~$XM = MX$}}.
\end{gather}
The invertible elements of~$\ro C(M)$ form the \emph{commutant
group}~$\ro C(M)^{×}$.

In the particular case where $M$~is cyclic, the commutant~$\ro C(M)$ is
the (commutative) polynomial algebra~$k[M]$. In this case, the IP1S
problem may be solved in a straightforward way~\cite{MPG2013}. We are
interested here in the general case, where $\ro C(M)$~is usually not a
commutative algebra.

\begin{prop}\label{prop:structure-sym}
Let~$M$ be any matrix.
\begin{enumerate}
\item \label{it:sym-inv} The set~$\ro S(M)$ contains an invertible
matrix~$T$.
\item \label{it:sym-comm} For any invertible~$T ∈ \ro S(M)$ and any~$A$,
the matrix $TA$~belongs to~$\ro S(M)$ if and only if $TA$~is symmetric
and $A ∈ \ro C(M)$.
\item Any symmetric pencil with characteristic endomorphism~$M$ is of the
form~$S_{λ} = TA (λ + M)$ where $A ∈ \ro C(M)$. Moreover, it is regular
if and only if $A$~is invertible.
\item Let~$S_{λ} = TA (λ + M)$ be a symmetric pencil and~$X ∈ \ro C(M)$.
Then
\begin{equation*}
\transpose{X} S_{λ} X = T \:(X^{⋆} A X)\: (λ + M), \quad
\text{where $X^{⋆} = T^{-1} \transpose{X} T ∈ \ro C(M)$.}
\end{equation*}
\item Let~$A ∈ \ro C(M)$. Then $\transpose{(TA)} = TA^{⋆}$. In
particular, $TA$~is symmetric if, and only if, $A = A^{⋆}$.
\end{enumerate}
\end{prop}

\begin{proof}
Point~\itemref{it:sym-inv} is explicitly proven in Prop.~\ref{prop:big-T}
below. Assuming that $TA$~is symmetric, point~\itemref{it:sym-comm}
follows from
\begin{equation}
\transpose{(TAM)} \;=\; \transpose{M}\, \transpose{(TA)}
 \;=\; \transpose{M}\,TA = TMA;
\end{equation}
since $T$~is invertible, it is cancellable in the resulting equation~$TMA
= TAM$.
\end{proof}

We call~$A^{⋆}$ the \emph{adjoint} of~$A$ and say that $A$~is
\emph{self-adjoint} if~$A^{⋆} = A$. (Since we don't plan to equip the
field~$k$ with an involution, no confusion with the usual meaning is
possible).

\begin{thm}\label{thm:IP1S-congruence}
The congruence problem for regular symmetric pencils is equivalent to the
following: given two invertible self-adjoint matrices~$A, A' ∈ \ro C(M)$,
compute a matrix~$X ∈ \ro C(M)$ such that~$X^{⋆} A X = A'$.
\end{thm}

\subsection{The commuting space of a Jordan matrix}%<<<2

By using the primary decomposition of~$M$ (= Chinese remainders) as well
as an extension of scalars, we may assume that the minimal polynomial
of~$M$ is a power of a prime polynomial. Using an extension of scalars, we
assume that it is a power of~$x$.

In this case, $M$~decomposes as Jordan blocks as follows: for each
integer~$u$, let~$H_u$ be the companion matrix of the polynomial~$x^u$.
Then there exists a decreasing sequence of integers~$n_1 ≥ … ≥ n_r$ such
that~$∑ n_i = n$ and $M$~is block-diagonal, with the $i$-th block being
equal to~$H_{n_i}$.

For each integers~$u ≥ v$, define matrices~$J_{u,v}$ and~$J_{v,u}$, of
respective sizes~$u × v$ and~$v × u$, by the block decompositions:
\begin{equation}
J_{u,v} = \mat{ \mathrm{id}(v) \\ 0^{v ×(u-v)}},\quad
J_{v,u} = \mat{ 0^{(u-v)× v } & \mathrm{id}(v)}.
\end{equation}

\begin{prop}\label{prop:rel-HJ}
Let~$u, v, w$ be three integers.
\begin{enumerate}
\item $H_{u} J_{u,v} = J_{u,v} H_{v}$.
\item The space of all matrices~$A$ of size~$u × v$ such that~$H_u A = A
H_v$ is exactly $k[H_u] J_{u,v}$.
\item \label{it:Juw} $J_{u,v} J_{v,w} = H_v^{d} J_{v,w}^{}$ where $d$~is
the distance between~$v$ and the interval~$[u,w]$. In particular,
$J_{u,v} J_{v,u} = H_{u}^{\abs{u-v}}$.
\end{enumerate}
\end{prop}

\begin{prop}\label{prop:structure-commutant}
Let~$M$ be the block-diagonal matrix with diagonal blocks~$H_{n_i}$.
The commuting space of~$M$ is the space of
all block matrices~$A = (A_{i,j})$, where $A_{i,j}$~belongs
to~$k[H_{n_i}] J_{n_{i}, n_{j}} = J_{n_i, n_j} k[H_{n_j}]$.
\end{prop}

Each entry~$A_{i,j}$ may be written as a polynomial
\begin{equation}
A_{i,j} = a_{i,j} (H_{n_i}) J_{n_i,n_j} = J_{n_i,n_j} a_{i,j} (H_{n_j})
\end{equation}
where $a_{i,j}(H) ∈ k[H]/H^{n_j}$.
We simplify the notation and write~$A = (a_{i,j})$ where~$a_{i,j} ∈
k[H]$. We note however that elements of~$\ro C(M)$ do not multiply as
matrices with coefficients in~$k[H]$, due to the relations
of Prop.~\ref{prop:rel-HJ}. An easy way to perform the computations is
given in Prop.~\ref{prop:phantom} below.

\subsection{The adjunction involution}%<<<2

We now describe the involution~$A ↦ A^{⋆}$ of the commuting space~$\ro
C(M)$.

\begin{prop}\label{prop:big-T}
For each integer~$u$, write~$T_u$ for the anti-identity matrix of
size~$u$.
\begin{enumerate}
\item $T_u$~is invertible and both~$T_u$ and~$T_u H_u$ are symmetric.
\item $\transpose{J_{u,v}} T_u = T_v J_{v,u}$.
\item Let~$T$ be the block-diagonal matrix with diagonal blocks equal
to~$T_{n_i}$. Then $T ∈ \ro S(M)^{×}$.
\end{enumerate}
\end{prop}


\begin{prop}
Let~$A = (a_{i,j}(H)) ∈ \ro C(M)$. Then \[ A^{⋆} = (a_{ji}(H)). \]
In particular, $TA$~is symmetric if, and only if, $a_{i,j} = a_{ji}$
in~$k[H]/H^{\min (n_i, n_j)}$.
\end{prop}

\subsection{Structure of the commutant group}%<<<2


\begin{lem}\label{lem:det-1block}
Let~$s$ be such that~$n_1 = … = n_s > n_{s+1}$.
Define~$n'_i = n_i - 1$ if~$i ≤ s$, and~$n'_i = n_i$ if~$i > s$.

For any matrix~$A ∈ \ro C(M)$ given by the block decomposition~$A =
(a_{i,j}(H_{n_i}) J_{n_i,n_j})$, define~$A'$ as the square matrix of
size~$n - s$ given by the blocks~$(a_{i,j}(H_{n'_i}) J_{n'_i,
n'_j})$.

Then \[ \det A \;=\; \det (a_{i,j}(0))_{1 ≤ i,j ≤ s} · \det A'. \]
\end{lem}


\begin{proof}
Let~$σ$ the unique permutation of~$[1,n]$ such that~$σ(n_i) = i$ for~$i =
1, …, s$, and~$σ$ is increasing on all other indices. Then the matrix
$A^{σ}$ deduced from~$A$ by applying~$σ$ both on the lines and the
columns of~$A$ is lower block-triangular, with two diagonal blocks
respectively equal to the matrix~$(a_{i,j}(0))_{i,j ≤ s}$ and to~$A'$.
Since~$\det A = \det A^{σ}$, this proves the lemma.
\end{proof}

For each $u, v$, the \emph{big block} of index~$(u, v)$ of~$A$ is the
matrix~$(a_{i,j})$, where $i,j$~run over the range where~$n_i = u$
and~$n_j = v$. The next proposition states that $A$~is invertible if and
only if all its diagonal big blocks are invertible as matrices with
coefficients in~$k[H]/H^{n_i}$.

\begin{prop}\label{prop:det-bigblock}
Let~$A = (a_{i,j}(H)) ∈ \ro C(M)$. Then
\[ \det A \;=\; ∏_{d ≥ 1} \det (a_{i,j}(0) | n_{i} = n_j = d)^{d} \]
where $a_{i,j}(0)$~is the image modulo~$H$ of~$a_{i,j} ∈ k[H]$.
\end{prop}

\begin{proof}
By induction on~$n_1$, with Lemma~\ref{lem:det-1block} providing the
induction step. The base case~$n_1 = 1$ corresponds to~$k[H] = k$ and
therefore~$A = (a_{i,j}(0))$.
\end{proof}

\begin{prop}\label{prop:structure-gl}
The commutant group~$\ro C(M)^{×}$ is
generated by the following matrices:
\begin{enumerate}
\item big-block-diagonal matrices, i.e. matrices whose only non-zero big
blocks are those on the diagonal;
\item small-block transvection matrices.
\end{enumerate}
\end{prop}

\begin{proof}
By Prop.~\ref{prop:det-bigblock}, an invertible matrix~$A ∈ \ro C(M)$ has
all its diagonal big blocks invertible. Therefore we may apply the
Gaussian elimination algorithm to factor $A$~as a product~$A = LU$, where
$L$~is lower triangular with diagonal elements~$1$, and $U$~is upper
triangular.
\end{proof}

\section{Classification of local symmetric pencils}%<<<1
\subsection{Symmetric pencils as symmetric bilinear forms}%<<<2

As in the previous section, we assume that $M$~is a Jordan matrix
determined by integers~$n_1 ≥ … ≥ n_r$. Let~$R = k[H]/H^{n_1}$.

The next proposition gives an easy way to compute the congruence action
of the commutant group~$\ro C(M)^{×}$ on self-adjoint matrices, by way of
ordinary matrix multiplication in the matrix algebra~$R^{r×r}$.

\begin{prop}\label{prop:phantom}
For any self-adjoint matrix~$A ∈ \ro C(M)$, define the symmetric
matrix~$Φ(A)$ with coefficients in~$R$ by
\[ Φ(A) = (a_{i,j} H^{n_1 - n_j}) ∈ \ro S_r(R). \]
Likewise, for any invertible~$X ∈ \ro C(M)^{×}$, define
\[ Ψ(X) = (x_{i,j} H^{\max (n_1-n_j, 0)}) ∈ GL_r(R). \]
Then, for all~$A$ and~$X$, the following relation holds (as matrices with
coefficients in~$R$):
\[ Φ(X^{⋆}\:A\:X) = \transpose{Ψ(X)}\: Φ(A)\: Ψ(X) ∈ \ro S_r(R). \]
\end{prop}

\begin{proof}
We first note that the diagonal big blocks of~$Ψ(X)$ are equal to those
of~$X$, which are invertible. Since the big blocks above the diagonal
of~$Ψ(X)$ are all divisible by~$H$, this proves that $Ψ(X)$~is
invertible.

By Prop.~\ref{prop:structure-gl}, it is enough to check
Prop.~\ref{prop:phantom} when $X$~is big-block-diagonal or a transvection
matrix. The first case is obvious, and the second one
follows from the multiplication relations of the matrices~$J_{u,v}$
given in Prop.~\ref{prop:rel-HJ}~\itemref{it:Juw}.
\end{proof}

For example in the case of two big blocks: assume~$n_1 > n_2$, $e = n_1 -
n_2$, and let
\[ A = \mat{A_{11} & A_{12}\\ \transpose{A_{12}} & A_{22}}, \quad
X = \mat{X_{11} & X_{12}\\ X_{21} & X_{22}}. \]
According to Prop.~\ref{prop:phantom}, $Φ(A)$ and~$Ψ(X)$ are defined as
\[ Φ(A) = \mat{A_{11} & H^e A_{12}\\ H^e \transpose{A_{12}} & H^e A_{22}},
\quad
Ψ(X) = \mat{X_{11} & H^e X_{12}\\ X_{21} & X_{22}}. \]
and $X^{⋆} A X$~may be easily computed as the symmetric matrix such
that~$Φ(X^{⋆}AX) = \transpose{Ψ(X)} Φ(A) Ψ(X)$.

\subsection{Classical results on symmetric bilinear forms}%<<<2

In this paragraph, $R$~is the local ring~$R = k[H]/H^n$.
Let~$\acco{1, δ}$ be a set of representatives for the
quotient~$R^{×}/(R^{×})^2$.

\begin{prop}\label{prop:local-diag}
Let~$A$ be a regular symmetric matrix with entries in~$R$. Then $A$~is
congruent over~$R$ to either the identity matrix or the diagonal
matrix~$(1, …, 1, δ)$.
\end{prop}

\begin{proof}
The Gram orthogonalization algorithm works; cf.
\cite[I(3.4)]{milnorhusemoller} or \cite[92:1]{omeara}.
\end{proof}

\subsection{Classification of regular symmetric pencils}%<<<2


\begin{prop}\label{prop:bb-diag}
Let~$A$ be an invertible, self-adjoint element of~$\ro C(M)$. Then there
exists $X ∈ \ro C(M)^{×}$ such that $X^{*} A X$~is big-block-diagonal.
\end{prop}


\begin{proof}
We prove this by induction on the number of big blocks of~$A$, using Gram
orthogonalization on the big blocks of~$A$. Let~$B$ be the first diagonal
big block of~$A$ and~$e$ be the first non-zero element~$n_1 - n_i$. We
may then write $A$~as a block matrix
\begin{equation}
Φ(A) = \mat{B & H^{e} C\\ H^e\transpose{C} & H^e A'}.
\end{equation}
Since $A$~is invertible, all its diagonal big blocks are invertible; in
particular the matrices~$B$ and $A'$~are invertible. By the induction
hypothesis, there exists~$X'$ such that~$Δ = (X')^{*} A' X'$ is
big-block diagonal.

We then define
\begin{equation}
Ψ(X) = \mat{1 & -H^e B^{-1} C\\0 & X'}\quad\text{and see that}\quad
Φ(X^{⋆} A X) = \transpose{Ψ(X)} Φ(A) Ψ(X) = \mat{B & 0\\0 & H^e Δ}.
\end{equation}
\end{proof}

We note that the regularity hypothesis on~$A$ is essential for
Prop.~\ref{prop:bb-diag}. As a counter-example, assume that the Jordan
sequence is~$n_1 > n_2$ with~$n_2 ≥ 2$, and let~$A = \mat{H_{n_1} &
J_{n_1,n_2} \\ J_{n_2,n_1} & H_{n_2}} = \mat{H&1\\1&H}$.
Then no matrix of the form~$Ψ(X)$ diagonalizes~$Φ(A) = \mat{H&H\\H&H^2}$
in~$R^{2×2}$, and therefore $A$~is not not big-block diagonalizable by a
matrix commuting with~$M$.

\begin{thm}\label{thm:diag}
Let~$A$ be an invertible, self-adjoint element of~$\ro C(M)$. Then $A$~is
congruent to a diagonal matrix, where each diagonal big block is either
the identity matrix, or the diagonal matrix~$(1, …, 1, δ)$.
\end{thm}

\begin{proof}
Use propositions~\ref{prop:bb-diag} and~\ref{prop:local-diag}.
\end{proof}


\section{In characteristic two}%<<<1
\subsection{Action of the commutant on diagonal matrices}

Let~$u$ be an integer. We define a $k-$linear map~$λ_u$ from $u ×
u$-matrices to~$R_u = k[H]/H^u$ in the following way:
\begin{equation}
λ_u((a_{i,j})_{i,j=1,…,u}) = ∑_{i=1}^{u} a_{i,i} H^{i-1}.
\end{equation}

\begin{prop}
Let~$φ: R → R$ be the Frobenius map, defined by~$φ(∑ x_i H^i) = ∑
x_i^2 H^i$. Then for all~$x ∈ k[H_u]$,
\begin{equation}
λ_u (\transpose{x} \, A\, x) = φ(x)\, λ_u(A).
\end{equation}
\end{prop}


\begin{proof}
Given that $λ_u$~is $k$-linear, it is enough to prove that
$λ_u(\transpose{H_u} \, A\, H_u) = H λ_u(A)$.
\end{proof}


\begin{lem}
Let~$u, v$ be two integers and~$A ∈ R^{u × u}$.
\begin{equation}
λ_v ( \transpose{J_{u,v}}\, A\, J_{u,v}) \;=\;
H^{\max (v-u, 0)}\,λ_u(A).
\end{equation}
\end{lem}


\begin{prop}
Let~$n_1 ≥ … ≥ n_r$. For any symmetric matrix~$A$ written as
blocks~$A_{i,j}$ of size~$n_i × n_j$, define~$Λ(A)$ as the line matrix of
length~$r$ with coefficients in~$R_1 = k[H]/H^{n_1}$:
\begin{equation*}
Λ(A) = ( H^{n_1 - n_i}\, λ_{n_i} (A_{i,i}))_{i=1,…, r}.
\end{equation*}
Then, for any~$X ∈ \ro C(M)$:
\begin{equation}
Λ(\transpose{X}\,A\,X) = Λ(A)\, φ(Ψ(X)).
\end{equation}
\end{prop}


\begin{prop}
Let~$A = \mat{Δ_1 & TM \\ 0 & Δ_2}$ and~$X = \mat{X_{1,1} & X_{1,2}\\
X_{2,1} & X_{2,2}}$, where~$X_i ∈ \ro C(M)$ and~$Δ_i$ are diagonal.

Then $\transpose{X} A X$ has diagonal
\[ Λ(\transpose{X} A X) = ( Λ(Δ_1) φΨ(X{1,i}) + Λ(Δ_2) φψ(X_{2,i}) +
  Λ(\transpose{X_{1,i}} TM X_{2,i}))_{i=1,2}. \]
\end{prop}

\begin{thebibliography}{99}%<<<1
\bibitem{omeara} Timothy O'Meara, \emph{Introduction to quadratic forms}.
\bibitem{milnorhusemoller} H. Milnor, D. Husemoller, \emph{Symmetric
bilinear forms}.
\bibitem{MPG2013} G. Macario-Rat, J. Plût, H. Gilbert,
\emph{New Insight into the Isomorphism of Polynomial Problem IP1S
 and its Use in Cryptography}, ASIACRYPT 2013.
\end{thebibliography}%>>>1
\end{document}
